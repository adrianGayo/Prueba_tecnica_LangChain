{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adrián Gayo Andrés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado\n",
    "Se propone la creación de un prototipo con varios\n",
    "agentes que permita extraer publicaciones científicas de ArXiv y las publique en X.\n",
    "En el ITCL se busca desarrollar una herramienta basada en un sistema de agentes cuyo\n",
    "objetivo principal es permitir que los usuarios accedan a ArXiv a través de una API,\n",
    "utilizando una interfaz de chat que funcione mediante lenguaje natural. Además, el sistema\n",
    "debe ofrecer la posibilidad de que los usuarios publiquen, de forma sencilla y en cualquier\n",
    "momento, los contenidos de una o varias investigaciones en la plataforma X. Este sistema\n",
    "estará diseñado para ser semiautónomo, pero siempre bajo la supervisión directa del\n",
    "usuario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions in c:\\users\\adria\\anaconda3\\lib\\site-packages (4.11.0)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install arxiv\n",
    "# %pip install langchain\n",
    "# %pip install langgraph\n",
    "# %pip install chromadb --upgrade\n",
    "# %pip install cohere\n",
    "# %pip install tweepy \n",
    "# %pip install gradio\n",
    "# %pip install openai\n",
    "# %pip install -U langchain-community\n",
    "# %pip install -U langchain_openai\n",
    "# %pip install -U langchain_cohere \n",
    "# %pip install pymupdf\n",
    "# %pip install -U typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.15\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import langchain\n",
    "import langgraph\n",
    "import tweepy\n",
    "import openai \n",
    "import chromadb\n",
    "import cohere\n",
    "import gradio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXiV API prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quantum algorithm for Khovanov homology\n",
      "Mukkamala-Pereñiguez master function for even-parity perturbations of the Schwarzschild spacetime\n",
      "On the distinguishability of geometrically uniform quantum states\n",
      "Stochastic Calculus and Hochschild Homology\n",
      "Measured Hockey-Stick Divergence and its Applications to Quantum Pufferfish Privacy\n",
      "Ensemble control of n-level quantum systems with a scalar control\n",
      "Boundary Curvature Scalars on Conformally Compact Manifolds\n",
      "A dagger kernel category of complete orthomodular lattices\n",
      "Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration\n",
      "Decoherence of Schrödinger cat states in light of wave/particle duality\n"
     ]
    }
   ],
   "source": [
    "# Iinicializamos el cliente de arXiv\n",
    "arxiv_client = arxiv.Client()\n",
    "\n",
    "# Query de ejemplo con la palabra \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "  query = \"quantum\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "# Obtenemos los resultados de la búsqueda\n",
    "results = arxiv_client.results(search)\n",
    "\n",
    "# Mostremos los resultados.\n",
    "for r in arxiv_client.results(search):\n",
    "  print(r.title)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pdfs/Coming full circle -- A unified framework for Kochen-Specker contextuality.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# También podemos descargar el PDF de un artículo.\n",
    "paper = next(arxiv_client.results(search))\n",
    "# Creamos un directorio para almacenar los PDFs en caso de que no exista.\n",
    "os.makedirs(\"pdfs\", exist_ok=True)\n",
    "# Download the PDF to the PWD with a custom filename.\n",
    "paper.download_pdf(filename=\"pdfs/\" + paper.title + \".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X API prueba (tweepy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15.0\n",
      "<tweepy.auth.OAuth2UserHandler object at 0x000001FD65C43FE0>\n"
     ]
    },
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\nYou are not allowed to create a Tweet with duplicate content.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(auth)\n\u001b[0;32m     25\u001b[0m client_X \u001b[38;5;241m=\u001b[39m tweepy\u001b[38;5;241m.\u001b[39mClient(\n\u001b[0;32m     26\u001b[0m         consumer_key\u001b[38;5;241m=\u001b[39mtwitter_api_key,\n\u001b[0;32m     27\u001b[0m         consumer_secret\u001b[38;5;241m=\u001b[39mtwitter_api_secret,\n\u001b[0;32m     28\u001b[0m         access_token\u001b[38;5;241m=\u001b[39mtwitter_access_token,\n\u001b[0;32m     29\u001b[0m         access_token_secret\u001b[38;5;241m=\u001b[39mtwitter_access_token_secret,\n\u001b[0;32m     30\u001b[0m     )\n\u001b[1;32m---> 32\u001b[0m response \u001b[38;5;241m=\u001b[39m client_X\u001b[38;5;241m.\u001b[39mcreate_tweet(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHola mundo!\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweet publicado exitosamente: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:839\u001b[0m, in \u001b[0;36mClient.create_tweet\u001b[1;34m(self, direct_message_deep_link, for_super_followers_only, place_id, media_ids, media_tagged_user_ids, poll_duration_minutes, poll_options, quote_tweet_id, exclude_reply_user_ids, in_reply_to_tweet_id, reply_settings, text, user_auth)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    837\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m--> 839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/2/tweets\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39mjson, user_auth\u001b[38;5;241m=\u001b[39muser_auth\n\u001b[0;32m    841\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ):\n\u001b[0;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(method, route, params\u001b[38;5;241m=\u001b[39mrequest_params,\n\u001b[0;32m    130\u001b[0m                             json\u001b[38;5;241m=\u001b[39mjson, user_auth\u001b[38;5;241m=\u001b[39muser_auth)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:100\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(response)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(response)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFound(response)\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Forbidden\nYou are not allowed to create a Tweet with duplicate content."
     ]
    }
   ],
   "source": [
    "# Nos autenticamos en la API de Twitter\n",
    "load_dotenv()\n",
    "twitter_api_key = os.getenv(\"TWITTER_API_KEY\")\n",
    "twitter_api_secret = os.getenv(\"TWITTER_API_SECRET\")\n",
    "twitter_access_token = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "twitter_access_token_secret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "client_id = os.getenv(\"CLIENT_ID\")\n",
    "client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "redirect_uri = \"http://127.0.0.1:5000/callback\" \n",
    "print(tweepy.__version__)\n",
    "\n",
    "# v1.0\n",
    "# auth = tw.OAuth1UserHandler(twitter_api_key, twitter_api_secret, twitter_access_token, twitter_access_token_secret)\n",
    "# api = tw.API(auth, wait_on_rate_limit=True)\n",
    "# api.verify_credentials()\n",
    "# print(\"Autenticación exitosa\")\n",
    "# api.update_status(status=\"Hello, world!\")\n",
    "# print(\"Tweet publicado exitosamente\")\n",
    "\n",
    "# v2.0\n",
    "auth = tweepy.OAuth2UserHandler(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, scope=\"tweet.write\")\n",
    "print(auth)\n",
    "\n",
    "client_X = tweepy.Client(\n",
    "        consumer_key=twitter_api_key,\n",
    "        consumer_secret=twitter_api_secret,\n",
    "        access_token=twitter_access_token,\n",
    "        access_token_secret=twitter_access_token_secret,\n",
    "    )\n",
    "\n",
    "response = client_X.create_tweet(text=\"Hola mundo!\", user_auth=True)\n",
    "print(f\"Tweet publicado exitosamente: {response.data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206 ['Is Long Context All You Need? Leveraging LLM’s Extended\\nContext for NL2SQL\\nExperiments, Analysis and Benchmark Paper\\nYeounoh Chung\\nGoogle\\nGaurav T. Kakkar\\nGoogle\\nYu Gan\\nGoogle\\nBrenton Milne\\nGoogle\\nFatma Özcan\\nGoogle\\nABSTRACT\\nLarge Language Models (LLMs) have demonstrated impressive ca-\\npabilities across a range of natural language processing tasks. In\\nparticular, improvements in reasoning abilities and the expansion\\nof context windows have opened new avenues for leveraging these', 'of context windows have opened new avenues for leveraging these\\npowerful models. NL2SQL is challenging in that the natural lan-\\nguage question is inherently ambiguous, while the SQL generation\\nrequires a precise understanding of complex data schema and se-\\nmantics. One approach to this semantic ambiguous problem is to\\nprovide more and sufficient contextual information.\\nIn this work, we explore the performance and the latency trade-', 'In this work, we explore the performance and the latency trade-\\noffs of the extended context window (a.k.a., long context) offered\\nby Google’s state-of-the-art LLM (gemini-1.5-pro). We study the\\nimpact of various contextual information, including column exam-\\nple values, question and SQL query pairs, user-provided hints, SQL\\ndocumentation, and schema. To the best of our knowledge, this is\\nthe first work to study how the extended context window and extra', 'the first work to study how the extended context window and extra\\ncontextual information can help NL2SQL generation with respect to\\nboth accuracy and latency cost. We show that long context LLMs are\\nrobust and do not get lost in the extended contextual information.\\nAdditionally, our long-context NL2SQL pipeline based on Google’s\\ngemini-pro-1.5 achieve a strong performance with 67.41% on BIRD\\nbenchmark (dev) without finetuning and expensive self-consistency\\nbased techniques.\\n1\\nINTRODUCTION', 'benchmark (dev) without finetuning and expensive self-consistency\\nbased techniques.\\n1\\nINTRODUCTION\\nRecent advancements in LLMs have particularly focused on en-\\nhancing their retrieval and reasoning abilities and expanding their\\ncontext windows, thus broadening the scope of their potential ap-\\nplications. The ability to process and retain longer sequences of\\ninformation within the context window empowers LLMs to capture\\nnuanced dependencies and intricate relationships within the input', 'nuanced dependencies and intricate relationships within the input\\ndata, offering unprecedented possibilities for improved language\\nunderstanding and generation. One domain that stands to signifi-\\ncantly benefit from these advancements is Natural Language to SQL\\n(NL2SQL). NL2SQL is a challenging task that entails translating\\nnatural language questions into structured SQL queries that can\\nbe executed on a database. The inherent ambiguity of natural lan-', 'be executed on a database. The inherent ambiguity of natural lan-\\nguage questions, coupled with the necessity for a deep understand-\\ning of complex database schemas and semantics, makes NL2SQL\\na very challenging problem [8]. Recent works [4, 10, 26, 28] cre-\\nated NL2SQL pipelines involving schema linking, self-consistency\\nand self-corrections with many LLM calls. These solutions care-\\nfully create prompts that include various context information, and', 'fully create prompts that include various context information, and\\nuse Chain-of-Though (CoT)[25, 42] reasoning and/or in-context-\\nlearning.\\nIn this paper, we explore the potential of harnessing the extended\\ncontext window provided by Google’s long context LLMs (gemini-\\n1.5) to improve NL2SQL performance. The hypothesis is that the\\nlong-context LLMs with enhanced retrieval and reasoning abili-\\nties over the extended context window can address the semantic', 'ties over the extended context window can address the semantic\\nambiguity challenges with additional and appropriate contextual\\ninformation. Under this assumption , we conduct a detailed study\\non the impact of these techniques when used with a long-context\\nmodel.\\nTable 1: Performance comparison of different published NL2SQL meth-\\nods on BIRD dev. This excludes undisclosed methods from the leader-\\nboard.\\nMethod\\nEx Acc (%)\\nFine-tuned\\nSelf-consistency\\nCHASE-SQLa [28]\\n74.46\\n✓\\n✓\\nXiYan-SQLb [10]\\n73.34', 'Method\\nEx Acc (%)\\nFine-tuned\\nSelf-consistency\\nCHASE-SQLa [28]\\n74.46\\n✓\\n✓\\nXiYan-SQLb [10]\\n73.34\\n✓\\n✓\\nLong Context (ours)\\n67.41\\n✗\\n✗\\nDistilleryc[26]\\n67.21\\n✓\\n✓\\nE-SQL[4]\\n65.58\\n✗\\n✗\\nCHESS[32]\\n65.00\\n✓\\n✓\\nMCS-SQL[16]\\n63.36\\n✗\\n✓\\nSuperSQL[19]\\n58.5\\n✗\\n✓\\na2nd on the leaderboard as of December 17th, 2024. Long context NL2SQL pipeline was\\nused as one of the three candidate generators for its tuned self-consistency (multiple\\nchoice and pick)\\nb1st on the leaderboard as of December 17th, 2024', 'choice and pick)\\nb1st on the leaderboard as of December 17th, 2024\\nc6th on the leaderboard as of December 17th, 2024\\nTable 1 compares the execution results accuracy (Ex Acc) of\\npublished NL2SQL methods using BIRD benchmark [21], the most\\npopular benchmark for NL2SQL testing. Our Long Context pipeline\\nyields very competitive results without any fine-tuning and with-\\nout generating multiple answer candidates (self-consistency). It is\\nshown that LLMs, especially the smaller capacity models benefit', 'shown that LLMs, especially the smaller capacity models benefit\\nsignificantly from fine-tuning as it helps the models focus on rele-\\nvant patterns in specific data domanins and SQL generation [6, 24].\\nWhile fine-tuning has been a dominant approach, in-context learn-\\ning (ICL) using the latest LLMs is gaining traction as a preferable al-\\nternative, matching the performances of fine-tuned models [27, 29].\\nICL does not require re-training and updating the model parameters', 'ICL does not require re-training and updating the model parameters\\nand avoid overfitting to specific data domains. This is particularly\\nappelaning to a production NL2SQL system where it serves many\\ncustomers across many specialized domains.\\nSelf-consistency is another popular technique used in the state-\\nof-the-arts NL2SQL systems [7, 9, 10, 28, 32, 35]. The idea is to\\ngenerate multiple output candidates and pick the most likely one', 'generate multiple output candidates and pick the most likely one\\naccording to some rules (e.g., majority voting) or a fine-tuned picker\\n1\\narXiv:2501.12372v1  [cs.DB]  21 Jan 2025', 'Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, and Fatma Özcan\\nmodel. This works nicely due to the stochastic nature of LLMs,\\nwhere different models (and even the same model) generates vary-\\ning outputs for the same input. Self-consistency is orthogonal to\\nour work, and one can combine the two [28] for higher accuracy\\n(see Section 6.2 for more discussion). In this work we focus on\\nidentifying and providing additional contextual information for', 'identifying and providing additional contextual information for\\nICL, leveraging extended context window of long-context LLM.\\nE-SQL is another method that does not employ fine-tuning and\\nself-consistency. Instead, E-SQL focuses on improved mapping\\nbetween user question and the relevant schema elements (a.k.a.,\\nschema linking). It modifies/enriches the original question ex-\\nplicitly with relevant schema elements (table and column names,', 'plicitly with relevant schema elements (table and column names,\\nvalues) and conditions, so the model can avoid implicit question\\nto the relevant schema elements mapping during generation. Such\\ntechniques to improve schema linking, just like fine-tuning and\\nself-consistency, is orthogonal and complementary to our work.\\nIn this paper, we conduct extensive empirical evaluations on\\nestablished NL2SQL benchmarks, such as the BIRD and SPIDER', 'established NL2SQL benchmarks, such as the BIRD and SPIDER\\ndatasets, to investigate the impact of long context on NL2SQL per-\\nformance. We explore various techniques for effectively leverag-\\ning long context in the NL2SQL pipeline, including strategies for\\ncontext construction, prompt engineering, and agentic workflow\\ncreation. To the best of our knowledge, this is the first study ex-\\nploring the real potential and implications of utilizing the long', 'ploring the real potential and implications of utilizing the long\\ncontext LLM and its extended context window of millions of tokens\\nfor NL2SQL, yielding competitive peformance on popular bench-\\nmarks. Google gemini-1.5 is currently the only long-context LLM\\nthat supports millions of tokens, whereas other models support\\nup to 8k - 32k tokens in the context. Due to this constraints, prior\\nworks focused on squeezing the filtered relevant information onto', 'works focused on squeezing the filtered relevant information onto\\nlimited context sizes. Our findings demonstrate that long con-\\ntext can indeed serve as a powerful tool for enhancing a typical\\nLLM-based NL2SQL pipeline and without finetuning. By employ-\\ning Google’s gemini-pro-1.5 and its long context capabilities, our\\nNL2SQL pipeline achieves 67.41% accuracy on the BIRD-Bench\\ndev dataset without any fine-tuning, showcasing the potential of', 'dev dataset without any fine-tuning, showcasing the potential of\\nthis approach. Through detailed analyses, we provide insights into\\nthe mechanisms by which long context contributes to improved\\nNL2SQL generation, shedding light on the specific types of contex-\\ntual information that are most beneficial. In particular, we observe\\nthe following insights:\\n• We observe that having the correct table and columns, i.e.\\n100% recall, in the context is required for high quality SQL', '100% recall, in the context is required for high quality SQL\\ngeneration. The long context model does not get distracted\\nwith additional table information, i.e. when we have a large\\nnumber of irrelevant tables in the context (low precision)\\n• Our experiments show that adding many relevant examples\\n(by question similarity) selected from training data sets do\\nnot improve NL2SQL accuracy significantly, contrary to\\nwhat we expected from the many-shot-learning paper[2].', 'what we expected from the many-shot-learning paper[2].\\nInstead, examples that are constructed using similar SQL\\nconstructs – as the underlying dev dataset (but without\\nlooking at them) – and relevant schema elements from the\\nsame target database improve accuracy. With this obser-\\nvation, we generate many such examples synthetically for\\nmany-shot ICL for NL2SQL. It is also the first study of\\nmany-shot ICL for NL2SQL with hundreds of examples,', 'many-shot ICL for NL2SQL with hundreds of examples,\\nwhereas the prior ICL for NL2SQL works utilize a handful\\n(3-5) of demonstrations/examples .\\n• In our ablation study, hints have the highest impact on\\nNL2SQL accuracy, followed by column sample values, and\\nself-correction. While high-quality in-context examples im-\\nprove accuracy, they do not solve all problems in NL2SQL\\ngeneration.\\n• Providing the SQL documentation in the long context does', 'generation.\\n• Providing the SQL documentation in the long context does\\nnot improve accuracy much, as the model has already seen\\nthese documents during training\\n• Latency increases (near) linearly with context size, hence\\nthere is a clear trade-off between latency and better accu-\\nracy.\\nIn the remainder of this paper, we present a comprehensive\\noverview of related work in Section 2, followed by a detailed de-\\nscription of our long-context based NL2SQL approach in Section3.', 'scription of our long-context based NL2SQL approach in Section3.\\nIn Section 4 we provide a detailed analysis of various techniques\\nand their impact on NL2SQL accuracy, as well as an ablation study.\\nFinally, we conclude with a discussion of the implications of our\\nfindings and directions for future research.\\n2\\nRELATED WORKS\\nEarly approaches in NL2SQL focused on rule-based and semantic\\nparsing methods [20, 37]. With the advent of deep learning, neural', 'parsing methods [20, 37]. With the advent of deep learning, neural\\nnetwork-based models have become dominant in NL2SQL research\\n[34, 41]. These models learn to map natural language questions to\\nSQL queries using large datasets of question-query pairs. These\\nmodels treat NL2SQL as a machine translation task, using encoder-\\ndecoder architectures to translate natural language questions into\\nSQL queries. More recently, LLMs with human-like ability to under-', 'SQL queries. More recently, LLMs with human-like ability to under-\\nstand and generate texts is leading the significant improvement in\\naccuracy and efficiency in NL2SQL and similar tasks [9, 13, 14, 39].\\nLLMs can also learn from a few examples provided in context\\nat inference (few-shot in-context learning). In the NL2SQL do-\\nmain, prior research [27, 29] focused on leveraging this few-shot\\nin-context learning (ICL) approach to guide the SQL generation.', 'in-context learning (ICL) approach to guide the SQL generation.\\nThe few-shot examples for NL2SQL consist of question and SQL\\npairs and are typically filtered by question embedding similarity\\nto fit within the limited context size. In other problem domains,\\nmany-shot ICL enabled by the newly expanded context window is\\nshown to consistently outperform the few-shot approach [2].\\nA new long context benchmark [17] shows that the latest long-', 'A new long context benchmark [17] shows that the latest long-\\ncontext LLMs can match the capability of state-of-the-art retrieval\\nsystems, while under-performing on compositional reasoning tasks,\\nlike NL2SQL against specialized fine-tuned models. This benchmark\\nstudy covers NL2SQL and uses un-tuned gemini-1.5-pro long context\\nLLM. They leveraged the long context by putting all the DB tables\\nand also fixed few-shot examples as in [9]. While their NL2SQL', 'and also fixed few-shot examples as in [9]. While their NL2SQL\\npipeline under-performs against the original [9] with fine-tuned\\nmodel on SPIDER 1.0 benchmark[38], our long-context strategy\\noutperforms both [9, 17], by better leveraging the long context\\nwith appropriate information. [3, 22] also studied ICL with long\\ncontext for other non-NL2SQL domains. These studies show that\\nmany commercial long context LLMs struggle with processing all', 'many commercial long context LLMs struggle with processing all\\nthe information presented in the long context, where there is a\\n2', 'Is Long Context All You Need? Leveraging LLM’s Extended Context for NL2SQL\\nExperiments, Analysis and Benchmark Paper\\nbias towards certain locations (e.g., the end of window) [23]. Our\\nexperiences point to the opposite direction where gemini-1.5-pro\\nexhibits no such strong bias and over much larger context window.\\nAs time of this writing, Google Cloud gemini-1.5-pro supports up\\nto 2-million tokens in context over Vertex AI API, whereas OpenAI\\nGPT-4o supports 8k tokens over API.', 'GPT-4o supports 8k tokens over API.\\nSchema linking is critical for accurate NL2SQL generation [8],\\nand it links ambiguous user question to relevant schema elements.\\nSchema linking often relies on careful relevant table and column se-\\nlection [4, 32], and a recent study [26] showed that the latest LLMs\\ncan retrieve relevant schema elements from unfiltered database\\nschema (i.e., passing all DB tables without selection) during infer-', 'schema (i.e., passing all DB tables without selection) during infer-\\nence. We also pass the entire database tables to the long context\\nand make a similar observation.\\nOne can refine the output through a number of LLM calls, veri-\\nfying, fixing and rewriting the SQL outputs. It is also common to\\nsample multiple answer candidates and select (self-consistency)\\nthe most probable or consistent output to improve the quality\\n[7, 9, 10, 28, 32, 35]. In this work, we focus on leveraging the long', '[7, 9, 10, 28, 32, 35]. In this work, we focus on leveraging the long\\ncontext ICL for a typical NL2SQL pipeline – which is orthogonal\\nand can be used with self-consistency. [28] uses our long context\\npipeline generated candidates with a fine-tuned picker.\\nWe do not use more complex prompting strategy, like Chain-\\nof-Thought (CoT)[25, 31, 36, 42], as its contribution is negligible\\n(see Section ?? for more discussion). CoT prompting is shown to', '(see Section ?? for more discussion). CoT prompting is shown to\\nimprove the performance of LLMs on arithmetic and commonsense\\nreasoning tasks. It often comes with a few demonstrations of in-\\ntermediate reasoning steps to guide the model to generate its own\\nreasoning chains.\\n3\\nLEVERAGING LONG CONTEXT FOR NL2SQL\\nFigure 1: Long Context NL2SQL Pipeline. Leveraging long\\ncontext LLMs can make the retrieval step less critical and the\\nagentic workflow (generate →fix & rewrite →verify) more', 'agentic workflow (generate →fix & rewrite →verify) more\\naccurate with extra contextual information.\\nIn this work, we focus on the additional contextual information\\nthat we can pass to the extended context window. We hypothesize\\nthat leveraging the extended context window and long-context\\nLLMs’ strong retrieval capability can make the retrieval step in\\nNL2SQL pipeline less critical; we can also make LLM agent calls to\\ngenerate, fix & re-write, and to verify more accurate. We explore', 'generate, fix & re-write, and to verify more accurate. We explore\\nvarious contextual information, like SQLite documentation, user\\nhints for clarification, extra column sample values and examples\\nfor many-shot in-context learning (ICL).\\nThis section is divided into three sub-sections, generate →fix\\n& rewrite →verify, corresponding to the three agentic workflow\\nFigure 2: Top K relevant table retrieval performance. The\\ntop-K relevant tables are selected from BIRD dev based on', 'top-K relevant tables are selected from BIRD dev based on\\nquestion embedding similarity, but without the knowledge\\nof a specific target DB as in some production environment.\\ncalls for generating and refining the output SQL, shown in Fig. 1.\\nEach sub-section details the appropriate contextual information\\nand techniques leveraging the extended context window per the\\ncorresponding LLM agent call.\\n3.1\\nGenerate\\nFocusing on the value of information rather than squeezing into', '3.1\\nGenerate\\nFocusing on the value of information rather than squeezing into\\nthe limited context window, we can provide a lot more contextual\\ninformation to address challenges in schema linking and seman-\\ntic errors stemming from ambiguous user question and complex\\ndata schema. We explore the following to assist the SQL generation.\\nAll database tables for high recall schema linking.\\nIn a typical NL2SQL pipeline, the relevant schema elements (table', 'In a typical NL2SQL pipeline, the relevant schema elements (table\\ncolumn names and values) are retrieved based on question embed-\\nding similarity. Accurate schema linking is a key to accurate SQL\\ngeneration [8]. Prior research works focus on improving schema\\nlinking accuracy via more accurate table and column selection\\n[4, 32]. Accurate schema retrieval can uplift the NL2SQL perfor-\\nmance, but there is still a lot of headroom (see Appendix A.2 for', 'mance, but there is still a lot of headroom (see Appendix A.2 for\\nmore discussion). In fact, table schema retrieval can have missing\\ntables and columns – which prevents LLMs from writing correct\\nSQL queries.\\nFigure 2 illustrates table schema retrieval simulation results mim-\\nicking production table retrieval setup. Note that the recall does\\nnot reach 100% and plateaus at around 82% with increasing 𝐾. In\\nsome production environments, user may ask about any databases', 'some production environments, user may ask about any databases\\nand without specifying which DB she is asking about, rendering\\ntable retrieval much more challenging (see Appendix A.1 for more\\ndetails on the simulation setup). In this simulation setup, the re-\\ntrieval service does not achieve perfect recall at 𝑘≥13, which is\\nthe maximum number of tables per database in BIRD dev. This\\nis because tables are retrieved from all DBs (100 tables) without', 'is because tables are retrieved from all DBs (100 tables) without\\nspecifying the target DB. In Section 4.3 we explore the impact of\\npassing all the tables per DB and across the DBs to ensure perfect\\nrecall at the cost of lower precision.\\nHere we improve schema linking via passing all database tables\\nto guarantee that all relevant elements are provided along with\\nlots of extra irrelevant ones. This raises a concern that the model\\n3', 'Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, and Fatma Özcan\\nmay get confused and “lost in the middle” of large context window\\n[8, 23]. However, it is not the case with the latest long-context LLM,\\ngemini-1.5; we leverage the latest long-context LLM’s “near-perfect“\\nretrieval capability [30].\\nProviding all DB tables via extended long context can perform\\nmuch better than table/column retrieval-based schema linking. In\\nSection 4.3 we compare our long-context schema linking approach', 'Section 4.3 we compare our long-context schema linking approach\\nto the baseline using top-K relevant tables, and in Section 5 we\\nstudy the increased context size and latency implications.\\nColumn description and sample values for improved column\\nselection. Prior research [34] shows that the column descriptors\\nand values as part of prompt can improve the accuracy of schema\\nlinking and column selection. [26] used both column descriptors,', 'linking and column selection. [26] used both column descriptors,\\ndata types along with a few sample column values in the input\\nprompt. These extra information is shown to help LLMs reasoning\\nabout the column references. In this work, we pass a lot more than\\njust a few sample column values for any non-trivial ‘text‘ columns.\\nIn Section 4.6, we demonstrate how such a brute-force approach can\\nfully leverage the model’s “near-perfect“ retrieval capability [30]', 'fully leverage the model’s “near-perfect“ retrieval capability [30]\\nto address ambiguous column reference and even literal errors.\\nUser provided hints for additional clarification. The extended\\ncontext window allows for additional instructions and clarification\\nfrom user to be passed in addition to the database schema and other\\ncolumn metadata.\\nQuestion: What is the total number of non-chartered schools in the county of Los', 'column metadata.\\nQuestion: What is the total number of non-chartered schools in the county of Los\\nAngeles with a percent (%) of eligible free meals for grades 1 through 12 that is\\nless than 0.18%?\\nEvidence: non-chartered schools refer to schools whose Charter = 0; K-12\\nmeans grades 1 through 12; percent of eligible free rate for K-12 = ‘Free Meal Count\\n(K-12)‘ * 100 / ‘Enrollment (K-12)‘\\nSQL: SELECT COUNT(T2.School) FROM frpm AS T1 INNER JOIN schools', '(K-12)‘ * 100 / ‘Enrollment (K-12)‘\\nSQL: SELECT COUNT(T2.School) FROM frpm AS T1 INNER JOIN schools\\nAS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.County = ’Los Angeles’ AND\\nT2.Charter = 0 AND CAST(T1.‘Free Meal Count (K-12)‘ AS REAL) * 100 /\\nT1.‘Enrollment (K-12)‘ < 0.18\\nFigure 3: An illustration of a hint (Evidence) prescribing the\\nnuanced column reference and the mathematical expression\\nneeded to answer a challenging question from BIRD dev\\ndataset.', 'needed to answer a challenging question from BIRD dev\\ndataset.\\nBIRD benchmark comes with hints for accurate SQL generation,\\nwhich clarifies the which column is being asked for and/or how to\\ncompute the value in question. Fig. 3 shows an example question\\nwith hints. Not all BIRD dev questions have hints. In production\\nNL2SQL services, the hints may come via multi-turn interactions\\nwith the user. We provide the hints, if available, as part of the user', 'with the user. We provide the hints, if available, as part of the user\\nquestion (Appendix A.4). In Section 5 we discuss the impact of hints.\\nSynthetic examples for many-shot ICL: While prior works on\\nICL for NL2SQL focused on selecting a few handful of examples\\n(3-5) for ICL [27, 29], we generate many examples for many-shot\\nICL. Many-shot ICL is shown to improve LLM generation quality\\non many different domains [2]. Here we generate and use tens', 'on many different domains [2]. Here we generate and use tens\\nand hundreds of examples with the extended context window. The\\nexamples are also generated at inference time, as prescribed in [28].\\nThe online generation allows bypassing the example retrieval pro-\\ncess and generate examples using schema elements more relevant\\nto the user question (we pass the entire schema for a given data-\\nbase along with LLM-based relevant column selection results). The', 'base along with LLM-based relevant column selection results). The\\nmodel generates synthetic examples including both the output SQL\\nand input natural language question. The generated SQL queries\\nfollow the target benchmark datasets with respect to the common\\nSQL features The structural similarities can help guide the model\\nto generate with similar SQL structures and features.\\nIn Section 4.5 we demonstrate how synthetic examples based\\nmany-shot ICL can improve the performance and discuss the per-', 'many-shot ICL can improve the performance and discuss the per-\\nformance and cost trade-off of example generation in Section 5.\\nRelevant SQLite documentation sections: The large extended\\ncontext window allows passing SQLite documentation in big chunks,\\nchapters and/or sections. Documentation writing tends to be lenghty\\nand contains a few illustrative examples. We do explore the feasi-\\nbility and the impact of documentation on performance and cost in\\nSection 4.7 and challenges in Section A.3.\\n3.2', 'Section 4.7 and challenges in Section A.3.\\n3.2\\nFix & Rewrite\\nWe allow the model to fix its errors and rewrite the SQL output based\\non execution. If the generated SQL query returns an error during\\nexecution, it triggers correction modules [4, 33] and retries until\\na valid SQL is returned for a fixed number of times . [29] showed\\nthat LLMs can correct for minor syntax errors in SQL. While the\\nexisting self-correction mechanisms and the latest LLMs themselves', 'existing self-correction mechanisms and the latest LLMs themselves\\nare effective in fixing SQL syntax errors, they do not address more\\nnuanced semantic errors like invalid literal references and incorrect\\njoin paths. Semantic errors may persist after syntax error correction\\n(e.g., referencing a valid literal value for a wrong column) and they\\nare hard to detect without the ground-truth query results. We check\\nfor potential semantic errors when the query returns empty result;', 'for potential semantic errors when the query returns empty result;\\nwe ask the model to rewrite the query given that the previous\\nquery resulted in an empty result set and with more extensive lists\\nof sample column values. There is a risk of false positive in relying\\non null (an empty result) for detecting semantic errors. In a case\\nthat the ground truth should be null, this would incur additional\\ncost of repeating the task. If the question is not ambiguous, the', 'cost of repeating the task. If the question is not ambiguous, the\\nmodel should return the same correct null query even with the\\nextra sample column values. If the question is ambiguous and the\\nmodel re-generates a non-empty result, then we prefer that over\\nthe null and move to the next verification step. Seeing the full list\\nof column values enables the model to select the right keyword and\\ncolumns and to reason about more correct alternative join paths.', 'columns and to reason about more correct alternative join paths.\\nPassing the full lists of sample column values inflate the context\\nsize significantly and requires long context and LLMs that can\\nretrieve accurately over the long context. We evaluate the impact\\nof this long context disambiguation strategy in Section 4.6. This\\ntechnique is expensive, but it can complement imperfect table &\\ncolumn selection and schema linking, which often leads to sub-\\noptimal NL2SQL performance [4].\\n4', 'Is Long Context All You Need? Leveraging LLM’s Extended Context for NL2SQL\\nExperiments, Analysis and Benchmark Paper\\n3.3\\nVerify\\nWe use the untuned gemini-1.5-pro LLM to verify the correctness of\\nthe final output. The model is given the entire database table schema\\nand question (with additional hints if provided) to judge (see Ap-\\npendix Fig. 10 for the prompt). We believe that fine-tuning a verifier\\nor a picker for self-consistency[28, 35, 40] can further improve the', 'or a picker for self-consistency[28, 35, 40] can further improve the\\naccuracy. While we focus on leveraging the long context ICL for a\\ntypical NL2SQL pipeline with simpler techniques (self-correction,\\nverification without fine-tuning), the aforementioned techniques\\nare orthogonal and can be used in conjunction. We demonstrate\\nhow this verification step can impact the final performance and the\\ngeneration cost in Section 5.\\n4\\nDETAILED ANALYSIS OF LONG CONTEXT\\nNL2SQL TECHNIQUES\\n4.1', 'generation cost in Section 5.\\n4\\nDETAILED ANALYSIS OF LONG CONTEXT\\nNL2SQL TECHNIQUES\\n4.1\\nEvaluation setup\\nWe use public Google Cloud (GCP) Vertex AI gemini API for all\\nour experiments. This is important for reproducibility of our exper-\\niments and analysis. We use the latest publicly available gemini-\\n1.5-pro-002 and gemini-1.5-flash-002 checkpoints. gemini-1.5 is a\\nlong-context LLM class with up to 2-million token context for pro\\nand 1-million token context for flash. Our test VMs and Vertex AI', 'and 1-million token context for flash. Our test VMs and Vertex AI\\nendpoints are located in the same GCP region (us-central1-b).\\nWe report the full pipeline performances on various NL2SQL\\nbenchmark datasets (BIRD [21], SPIDER 1.0 [38], KaggleDBQA [15]\\nand BEAVER [5]) in Section 4.2. We run our micro-benchmark\\nexperiments using BIRD dev as a representative dataset, since it\\ncontains the most number of questions with varying degrees of', 'contains the most number of questions with varying degrees of\\ndifficulties spanning over multiple tables with varying schema com-\\nplexity. BIRD is also currently the most popular NL2SQL benchamrk\\nwith a leaderboard (all the latest NL2SQL research publications use\\nthis dataset to compare their performances).\\nFor the performance and cost metrics, we use popular execution\\naccuracy (Ex Acc) and the accumulated tokens per request (𝐿),\\nand/or normalized latency per request (𝑇) as the absolute latency', 'and/or normalized latency per request (𝑇) as the absolute latency\\nmeasure behind the public API endpoints can vary from time to\\ntime – a single gemini-1.5-pro request latency with 8k-token is used\\nas a reference unit (𝑇=1.0); we report floating-point values to one\\ndecimal place, ignoring differences of less than 10%. Likewise, we\\nrefrain from reporting the monetary cost directly, which depends\\non the pricing model [11] that is subject to change. Instead, we', 'on the pricing model [11] that is subject to change. Instead, we\\nreport the accumulated number of tokens per request 𝐿, which is\\nhighly correlated to the total monetary cost and invariant to the\\npricing model change.\\nThere is a strong positive correlation between 𝐿and 𝑇, as dis-\\ncussed in Section 4.8, and we report both in the ablation study.\\nWe report the average metrics across requests with tight variance\\n(margin of error/variability less than 2%); if the observed variance', '(margin of error/variability less than 2%); if the observed variance\\nis large, we report the mean plus one standard deviation (e.g., ¯𝐿+𝜎)\\nto bound the majority of requests.\\nOur “full” pipeline employs all the information and the tech-\\nniques as discussed in Section 3 and Section 5; for the individual\\nexperiments we use the “baseline” pipeline to study specific ques-\\ntions being addressed. The baseline pipeline includes entire DB', 'tions being addressed. The baseline pipeline includes entire DB\\nschema, sample column values, instructions and hints, but excludes\\nself-correction, disambiguation, synthetic examples and no verifi-\\ncation.\\n4.2\\nBenchmark evaluation\\nTable 2: Benchmark performance and generation latency 𝑇𝑔\\nusing the full long-context NL2SQL pipeline and with gemini-\\n1.5 models.𝑇𝑔measures per request SQL output generation time,\\nincluding time spent on self-correction and retries.\\ngemini-1.5-pro\\ngemini-1.5-flash', 'including time spent on self-correction and retries.\\ngemini-1.5-pro\\ngemini-1.5-flash\\nEx Acc (%)\\n¯𝑇𝑔(T units/req)\\nEx Acc (%)\\n¯𝑇𝑔(T units/req)\\nBIRD dev\\n67.41\\n12.3\\n66.49\\n8.6\\nSPIDER 1.0 test\\n85.70\\n1.5\\n84.60\\n1.1\\nKaggleDBQA test\\n61.10\\n2.0\\n59.50\\n1.6\\nBEAVER dwa\\n47.7\\n371.3\\n45.5\\n426.3\\naOnly Data Warehouse 1 (dw) dataset with 48 questions is publicly available as of January\\n7th, 2025.\\nTable 2 shows the full pipeline evaluation results with various', '7th, 2025.\\nTable 2 shows the full pipeline evaluation results with various\\nbenchmark datasets. The popular benchmark datasets (BIRD dev,\\nSPIDER 1.0 test, KaggleDBQA test) provide a good set of questions\\nof mixed difficulties over multiple domains and put our perfor-\\nmance in perspective with other state-of-the-arts. Our approach\\nleveraging the extended context of gemini-1.5 model can yield very\\ncompetitive results in all the benchmarks, without techniques like', 'competitive results in all the benchmarks, without techniques like\\nfine-tuning and self-consistency (Table 1). The newer benchmark\\nfor enterprise warehouse data (BEAVER dw) is also interesting be-\\ncause the business-oriented questions and real enterprise tables\\nare more complex and larger – BIRD is the most complex among\\nthe three popular benchmarks, and on the average, its questions\\nhave 0.918 JOINs over 6.82 tables/DB; BEAVER questions have 4.25', 'have 0.918 JOINs over 6.82 tables/DB; BEAVER questions have 4.25\\nJOINs over 105.0 tables/DB. BEAVER is a new benchmark and is\\nstill in early stage. The best 1-shot Ex Acc on BEAVER (dw + 45\\nadditional questions) from [5] is 2.0 in PL/SQL using gpt-4o with\\nrelevant table schema retrieval and 0.0 without retrieval.\\nThe normalized generation latency 𝑇𝑔increases with both larger\\ncontext size and task difficulty (question and schema complexity), as', 'context size and task difficulty (question and schema complexity), as\\nit includes the self-correction and retries (with exponential back-off\\nand max retries per request set to 5). For instance, BEAVER takes\\nmuch longer to generate on average as our long context pipeline\\nproduces context size exceeding the limits (2-million tokens for pro\\nand 1-million tokens for flash) or results in semantically invalid\\n(empty) outputs, in which case the model retries a fixed number of', '(empty) outputs, in which case the model retries a fixed number of\\ntimes until it finds a better answer. For this reason, flash can take\\nlonger on average than pro.\\nIn general, gemini-1.5-pro is better at complex reasoning tasks\\nlike NL2SQL, whereas gemini-1.5-flash is more cost-effective (lower\\npricing per token). We have noticed that the latest releases of the\\ngemini models (released for public access on Sep. 24th, 2024) show\\nsignificant performance improvements, and most notably, flash', 'significant performance improvements, and most notably, flash\\nnow performs more comparably to pro on the select benchmarks.\\nThis makes the more cost-efficient flash a very attractive choice for\\nproduction NL2SQL.\\n5', 'Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, and Fatma Özcan\\nTable 3: Table retrieval (TBR) performance and BIRD dev\\nexecution accuracy with the baseline pipeline. Top-K table\\nretrieval (𝑘=1,7) was done across the specific target DB tables\\nper question; putting all tables (all tables/DB) guarantees\\nperfect recall\\n# tables (k)\\nk=1\\nk=7\\nall DB tables\\nall dev tables\\nEx Acc (%)\\n38.01\\n54.69\\n62.32\\n62.58\\nTBR Recall (%)\\n45\\n82\\n100\\n100\\nTBR Precision (%)\\n77\\n23\\nLow (< 35%)\\nLow (< 2%)', '38.01\\n54.69\\n62.32\\n62.58\\nTBR Recall (%)\\n45\\n82\\n100\\n100\\nTBR Precision (%)\\n77\\n23\\nLow (< 35%)\\nLow (< 2%)\\nContext size (tok/req)\\n2002.54\\n4627.68\\n7380.86\\n72619.96\\n4.3\\nSchema linking with all database tables\\nWe explore the impact of passing the entire table schema set col-\\nlected from entire databases. Schema linking is such a critical com-\\nponent that we strive to achieve a high-recall in production. With\\na long context model, we can pass a lot more table definitions and', 'a long context model, we can pass a lot more table definitions and\\nschema more loosely than the most relevant top-k tables. As with\\na standard practice, we retrieve the most relevant tables based on\\nquestion and full table creation statement with column values and\\ndescriptions, embedding similarity. All the benchmark datasets pro-\\nvide specific target DB domain per question, which is different from\\nthe production setup we assumed in Figure 2 for the illustration.', 'the production setup we assumed in Figure 2 for the illustration.\\nThis narrows the search space, but even so, getting the perfectly ac-\\ncurate retrieval results is often infeasible. The goal is to ensure near\\nperfect (higher) recall at the cost of lower precision by providing\\nmore tables, if not all, via long context .\\nThe results shown in Table 3 demonstrates how the model does\\nnot get confused with a large number of mostly irrelevant table', 'not get confused with a large number of mostly irrelevant table\\ndefinitions in the context; thus providing more tables rather than\\nsolely relying on inaccurate table retrieval mechanism could make\\nsense if the context size permits. The result (𝑘= 1) also re-affirms\\nthat schema linking is critical, since without all the ground truth\\ntables in the prompt (𝑘= 1) the model cannot generate good quality\\nSQL outputs. On the flip side, providing all tables from the target', 'SQL outputs. On the flip side, providing all tables from the target\\nDB ensures perfect TBR recall and results in higher Ex Acc. This\\nmeans that TBR recall is more important than precision or accuracy\\nas the model generation quality does not degrade much with many\\nirrelevant tables (93.12 irrelevant tables on the average). all DB\\ntables use up to 13 tables to each request depending on the target\\nDB; the average number of tables across different DBs is 6.82. Notice', 'DB; the average number of tables across different DBs is 6.82. Notice\\nthat the execution accuracy result for all dev tables (100 tables from\\nBIRD dev) is slightly higher (within 5% margin of error – we use\\nmedium temperature for LLM generation and Ex Acc vary slightly\\nper run), which indicates that the results are comparable. However,\\nthe average context size increases significantly as with all dev tables.\\nOn the one hand, it is better to include as many tables (e.g., user', 'On the one hand, it is better to include as many tables (e.g., user\\nhistory, all other tables within the target DB) to guarantee high TBR\\nrecall, which should be possible with long-context LLMs. On the\\nother hand, the latency cost of processing larger context is higher\\n(see Fig. 5) – and to some extent, adding more irrelevant tables\\nwould not help and justify the increase cost. Furthermore, some\\nproduction setting require processing hundreds of very wide tables,', 'production setting require processing hundreds of very wide tables,\\nwhich prevents putting all the tables in the context prohibitively\\nTable 4: BIRD dev Ex Acc (%) with the baseline pipeline &\\nselected similar examples from BIRD train dataset, 𝜎(train),\\nand ground-truth (+ GT)\\n(a) Varying number of similar train examples and GT inserted in the\\nmiddle\\n# train examples\\n0\\n5\\n20\\n50\\n100\\n𝜎(train)\\n61.60\\n63.17\\n62.71\\n62.58\\n62.52\\n𝜎(train) + GT\\n78.68\\n77.18\\n77.84\\n77.51\\n78.81\\nContext size (tok/req)\\n7380.86', '63.17\\n62.71\\n62.58\\n62.52\\n𝜎(train) + GT\\n78.68\\n77.18\\n77.84\\n77.51\\n78.81\\nContext size (tok/req)\\n7380.86\\n8000.67\\n9927.24\\n13808.71\\n20358.30\\n(b) GT inserted at different positions (normalized) within the context\\nwindow with selected 100 BIRD train examples\\nGT position\\n0.1\\n0.25\\n0.50\\n0.75\\n0.9\\n𝜎(train) + GT\\n77.77\\n78.29\\n78.1\\n78.42\\n78.16\\n(c) Changing the order of the examples block (100 BIRD train ex-\\namples with GT) in the context: beginning (before the instructions),', 'amples with GT) in the context: beginning (before the instructions),\\nmiddle (before the schema details), end (after the schema details)\\nBlock position\\nbeginning\\nmiddle\\nend\\n𝜎(train) + GT\\n78.62\\n78.42\\n78.62\\nexpensive (see Section 6.3 for more discussion). Thus, accurate re-\\ntrieval/filtering is still desirable and one should consider leveraging\\nlong-context to offset the imperfect retrieval mechanism for higher\\nrecall.\\n4.4\\nMany-shot ICL with example selection', 'recall.\\n4.4\\nMany-shot ICL with example selection\\nHere we evaluate the impact of many-shot in-context learning (ICL)\\nusing examples selected from BIRD train dataset. The examples are\\nretrieved based on the question embedding similarity.\\nTable 4(a) shows that the train examples did not provide much\\nboost and instead resulted in worse performance. This means that\\nthe model’s ability to learn from similar examples from BIRD train\\nis quite limited and can be distracted from them. Also notice that', 'is quite limited and can be distracted from them. Also notice that\\nthe impact of distraction seems to be limited, say going from 1\\nsimilar example (11k tokens) to 100 similar examples (50k tokens).\\nTo examine the model’s sensitivity to the many irrelevant train\\nexamples further, we injected 1 ground-truth example from BIRD\\ndev in the “middle” of the example blob (the average context size is\\nomitted as it should be almost identical to the previous table). While', 'omitted as it should be almost identical to the previous table). While\\nthe model can select the ground truth example among many other\\nirrelevant ones, the presence of those irrelevant train examples can\\nconfuse the model as opposed to just having that ground truth. The\\nrecall matters more than precision, but the model is still sensitive\\nto bad examples (low precision) to a limited degree – note that the\\naccuracy with 100 random examples and 1 ground truth is much\\nhigher than no examples at all.', 'accuracy with 100 random examples and 1 ground truth is much\\nhigher than no examples at all.\\nIt is also important to note that the model is robust to the position\\nand ordering of the ground-truth example. [23] showed that LLMs\\ntend to put more emphasis on the information in the beginning and\\nthe end, where the order of information in context matters and calls\\nproper ranking and ordering for RAG. As in Table 4(b-c), where\\n6', 'Is Long Context All You Need? Leveraging LLM’s Extended Context for NL2SQL\\nExperiments, Analysis and Benchmark Paper\\nthe GT is injected at different positions within the context window,\\nwe observe that with gemini-pro-1.5 the ordering of the examples\\n(and the schema information relocated by the relocation of the\\nexamples in (c)) has a negligible impact, and the long-context\\nLLMs can retrieve the relevant ground truth example (also the', 'LLMs can retrieve the relevant ground truth example (also the\\nschema information) from anywhere within its context window.\\nThe model’s ability to learn and generalize from random exam-\\nples is limited: a single relevant example is far better than having\\nmany random examples. While it is crucial to capture the relevant\\nexamples in the context (relevant examples boost the generation\\nquality), it is also very challenging to capture the relevant ones', 'quality), it is also very challenging to capture the relevant ones\\nbased on the user questions. In the next section, we also share our\\nexperience generating many synthetic examples per question to\\nskip the example selection step.\\n4.5\\nSynthetic example generation VS. example\\nselection\\nFigure 4: Question similarity-based example selection (𝜎) vs.\\nonline example generation. The baseline uses no examples;\\nexamples were retrieved from BIRD train dataset, dev dataset', 'examples were retrieved from BIRD train dataset, dev dataset\\nand synthetic generated examples. train + GT uses the re-\\ntrieved train examples and the ground-truth SQL from dev.\\nFig. 4 illustrates how synthetic examples improve the quality of\\ngenerated SQL outputs and compare with other types of examples\\nretrieved from different example pools. As with a standard practice,\\nthe example retrieval was done via question embedding similarity', 'the example retrieval was done via question embedding similarity\\nusing gecko embedding model. Note that using the ground-truth\\nSQL queries from dev dataset provides performance ceilings (𝜎(dev)\\nand 𝜎(train + GT)), but is not allowed without an oracle (i.e., passing\\nthe answer as part of the request prompt). The gap between 𝜎(dev)\\nand 𝜎(train + GT) indicates that providing the GT as one of the\\nmany examples is better than retrieving relevant examples from', 'many examples is better than retrieving relevant examples from\\nthe pool that contains the GT, due to the imperfect retrieval process.\\n𝜎(train) is the common practice where the relevant examples are\\nselected from available datasets, such as the training dataset (train).\\nProviding relevant train examples does as least as good as the base-\\nline (no example) with a slight improvement when fewer than 200\\nshots are provided. Interestingly, synthetic examples (𝜎(synthetic)', 'shots are provided. Interestingly, synthetic examples (𝜎(synthetic)\\nTable 5: BIRD dev Ex Acc (%) and extra accumulated tokens\\nfor correction. The eval baseline pipeline does not include self-\\ncorrection (SC); disambiguation refers to providing exntensive\\nsample column values as described in Section 3.2; filtered schema\\nis providing filtered relevant columns and values to re-write\\nCorrected SQL\\nTokens used for Correction\\nEx Acc (%)\\n¯𝐿(tok/req)\\n¯𝐿+ 𝜎(tok/req)\\nBaseline (w/o SC)\\n61.6\\n-\\n-\\nSC\\n64.80', 'Ex Acc (%)\\n¯𝐿(tok/req)\\n¯𝐿+ 𝜎(tok/req)\\nBaseline (w/o SC)\\n61.6\\n-\\n-\\nSC\\n64.80\\n2211.34\\n34573.67\\nSC + disambiguation\\n65.51\\n15754.64\\n334999.28\\nSC + filtered schema\\n65.84\\n2335.9a\\n35475.69\\nathis excludes the cost of running the LLM-based column selection, where multiple LLM\\ncalls are needed to extract relevant columns for given request\\nand original synthetic) provide significant uplift over 𝜎(train). The\\ngap between 𝜎(synthetic) and original synthetic indicates that one', 'gap between 𝜎(synthetic) and original synthetic indicates that one\\ncan be more economical with the context size (achieving higher per-\\nformance with a fewer examples) by filtering the relevant synthetic\\nexamples.\\n4.6\\nSelf-correction with entire schema and more\\ncolumn values\\nTable 5 shows how LLM’s self-correction can help with SQL genera-\\ntion and its refinement. We compare the LLM-based self-correction\\n(SC) strategy with the ones with more advanced techniques, disam-', '(SC) strategy with the ones with more advanced techniques, disam-\\nbiguation and filtered schema. disambiguation is the long-context\\nenabled technique where an extended lists of sample column values\\nare shown to the model upon detecting empty results from syntacti-\\ncally correct SQL; filtered schema uses the column selection results\\nmapping relevant schema elements per user request, as prescribed\\nin [32]. Both disambiguation and filtered schema can help LLMs', 'in [32]. Both disambiguation and filtered schema can help LLMs\\ncorrect for any invalid literal and column column references. One\\npotential issue with disambiguation is that the extra column values\\ncan distract the model, especially in the case of false positive detec-\\ntion (e.g., the answer should be null). We empirically show that it\\nhelps more and the technique performs slightly better or at least as\\ngood as SC. While comparable in performance, the techniques vary', 'good as SC. While comparable in performance, the techniques vary\\nin terms of the cost. The accumulated average number of prompt\\ntokens used for correction increases significantly for disambigua-\\ntion, as with the full schema and extra sample column values. In\\ngeneral. accurate column selection can uplift NL2SQL performance,\\nand the filtered schema does not increase the context size during\\ncorrection.\\nHowever, column selection process to prepare filtered schema', 'correction.\\nHowever, column selection process to prepare filtered schema\\ncomes with retrieval cost (see Appendix A.2 for more discussion).\\nfiltered schema is a great strategy, if accurate column selection pro-\\ncess is available. In our final long-context pipeline, we use SC +\\ndisambiguation to avoid extra retrieval/selection step and do every-\\nthing in-context. We report the mean tokens/request plus 1-S.D,\\nsince the variance is large and to show how much tokens we use', 'since the variance is large and to show how much tokens we use\\nfor the majority ( 68%) of the requests.\\n7', 'Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, and Fatma Özcan\\nTable 6: BIRD dev execution accuracy with SQLite document\\nchunks, measured with the baseline pipeline.\\n(a) Coarse-grained document chunks\\n# chunks\\n0\\n1\\n2\\n3\\nEx Acc (%)\\n61.84\\n61.54\\n61.08\\n61.67\\nContext size (tok/req)\\n7380.87\\n32703.21\\n42087.60\\n51308.69\\n(b) Fine-grained document chunks\\n# chunks\\n0\\n1\\n5\\n10\\n15\\nEx Acc (%)\\n61.84\\n60.43\\n61.86\\n61.54\\n61.15\\nContext size (tok/req)\\n7380.87\\n7610.66\\n10115.78\\n14244.70\\n17560.80\\n4.7', '61.84\\n60.43\\n61.86\\n61.54\\n61.15\\nContext size (tok/req)\\n7380.87\\n7610.66\\n10115.78\\n14244.70\\n17560.80\\n4.7\\nIn-context learning with SQLite\\ndocumentation\\nWe test if the model can actually learn and improve its generation\\nquality with SQLite documentation. We downloaded the entire\\nSQLite documentation from its official homepage [1]. The original\\ndocumentation comprises 16 million tokens across 792 HTML files,\\neach covering a distinct theme. We applied two strategies to split', 'each covering a distinct theme. We applied two strategies to split\\nthe entire documentation into chunks so that we can augment the\\nprompts with relevant information for ICL. The coarse-grained\\nchucking strategy split the documentation by HTML file, while the\\nfine-grained chucking strategy further separate each file by sections\\nwhich ends up with 4125 smaller chunks in total. Similar to example\\nretrieval, we embed the natural language questions and document', 'retrieval, we embed the natural language questions and document\\nchunks into vectors with the Gecko text embedding model [18]\\nand employ nearest neighbor search to identify the most relevant\\ndocument chunks to the given question. We could further compress\\nthe context via summarization, but decided not to, since it drops\\nimportant details, like syntax rules and examples.\\nWe show the execution accuracy and context size for document', 'We show the execution accuracy and context size for document\\nretrieval in table 6. The retrieval is challenging because the natural\\nlanguage question does not reveal the full structure and features of\\nthe corresponding SQL query; furthermore writing a correct SQL\\nquery requires a combination of concepts from multiple sections\\nfrom the documentation. However, we believe that the less than\\nideal retrieval process is not the reason why the documentation', 'ideal retrieval process is not the reason why the documentation\\nadds little to no value to generation. It is actually the nature of the\\nerrors that the LLMs make, which are mostly semantic errors (e.g.,\\nexecutable queries returning semantically irrelevant results).\\nThe Gemini-1.5-Pro model is already well-versed with the SQLite\\nsyntax and function details that are illustarted in the documenta-\\ntion. The SQLite documentation is likely already in the pretrain', 'tion. The SQLite documentation is likely already in the pretrain\\nmixture. Furthermore, the model can self-correct itself for syntactic\\nerrors based on the error message and without any documenta-\\ntion or examples. A recent work [32] based on GPT-4 also points\\nout that there is no obvious errors based on simple syntax misuse\\n(e.g., function names), but rather more subtle formatting (e.g., does\\nnot follow required date format) and semantic errors (output SQL', 'not follow required date format) and semantic errors (output SQL\\nis executable but incorrect w.r.t. the request). Reading the SQLite\\ndocumentation would not help with such semantic errors – be-\\ncause the model is already producing executable queries that are\\nsemantically incorrect. While there is no accuracy gain, putting the\\nTable 7: BIRD dev Ex Acc (%) and average generation and\\nverification latency using the baseline pipeline with gemini-\\n1.5-pro and gemini-1.5-flash\\ngemini-1.5-pro', '1.5-pro and gemini-1.5-flash\\ngemini-1.5-pro\\ngemini-1.5-flash\\nSingle generation latency (sec)\\n1.0\\n0.8\\nSingle verification latency (sec)\\n0.9\\n0.2\\nFigure 5: Single LLM request latency by different context\\nsizes. Notice that axes are in log-scale, and there is a step-\\nwise linear relationship between latency and context size.\\nBoth gemini-1.5-pro and gemini-1.5-flash models suffer from\\nincreased and high-variable latency per request beyond the', 'increased and high-variable latency per request beyond the\\ncontext size of > 32𝑘tokens. The fitted lines represent best\\nlinear-fit between context size and latency measures.\\ndocumentation chunks increase the context size, thus the latency,\\nsignificantly.\\n4.8\\nLong context and latency relationship\\nIn this section, we look at the context size (tokens) and latency\\nrelationship. Fig. 5 illustrates that there is near-linear relationship', 'relationship. Fig. 5 illustrates that there is near-linear relationship\\n(strong positive correlation 𝑅2=92.6). It is interesting to note that\\nthe latency and the variance starts increasing significantly beyond\\ncontext size >32k. The larger context LLMs require inference com-\\nputation scattered across more host and accelerators, which incurs\\nmore queueing delay. While we expect the smaller gemini-1.5-flash\\nvariant to have lower latency, due to this queueing delay and re-', 'variant to have lower latency, due to this queueing delay and re-\\nsource allocation differences it also suffers from increased latency\\nat the long tail. The generation latency increases significantly (>>\\n4 seconds) with larger context size (>32k tokens), thus we should\\nincrease the context only if there is a clear gain in generation qual-\\nity. The average measures between context size and latency are\\nlinearly correlated, which makes it easy to model.', 'linearly correlated, which makes it easy to model.\\nTable 7 illustrates the average latency difference for the two\\ngemini-1.5 model variances, large and more expensive pro and small\\nand cheaper flash. The difference in verification latency is more pro-\\nnounced because the context size is smaller, whereas the generation\\nwith larger context pro and flash requires more prefill computation,\\nwhich results in increased and similar queueing delay. Both models', 'which results in increased and similar queueing delay. Both models\\nexperience increased and similar average generation latency. For\\n8', 'Is Long Context All You Need? Leveraging LLM’s Extended Context for NL2SQL\\nExperiments, Analysis and Benchmark Paper\\nthe single verification latency (indicative of the smaller context\\nlatency) flash is almost 75% faster than pro.\\n5\\nABLATION STUDY\\nOverall, we focus on identifying useful information for improving\\nNL2SQL performance – and measure the impacts of the contex-\\ntual information in terms of both performance and cost. We ex-', 'tual information in terms of both performance and cost. We ex-\\nclude the rules and SQLite documentation chunks from the ablation\\nstudy, since their separate contributions are negligible. The rules\\nare fixed and included as part of the instructions. We also skip\\npopular techniques, like Chain-of-Thought (CoT) prompting and\\nself-consistency. CoT can help LLMs by breaking down a complex\\ntask into sub-tasks enabling multi-step reasoning to solve complex', 'task into sub-tasks enabling multi-step reasoning to solve complex\\nreasoning task, like NL2SQL [25, 31]; however, we observe that\\nCoT does not improve the final output quality while increasing the\\ncontext size by a few thousands tokens.\\nTable 8 reports the generation performance (Ex Acc) along with\\naccumulated context size (tokens/req) and latency (sec/req) per user\\nrequest. The study is done adding one technique or extra contex-\\ntual information at a time. The techniques and information that', 'tual information at a time. The techniques and information that\\nare more commonly used for NL2SQL are added earlier (schema,\\nhints, column samples and self-correction, respectively) and our\\nlong-context pipeline specific components are added later (disam-\\nbiguation and synthetic examples).\\nBecause the latest long-context LLM, gemini-pro-1.5, has really\\nstrong retrieval capability, we see the overall performance increase\\nwith more added information. However, when we break down the', 'with more added information. However, when we break down the\\nquestions from BIRD dev into sub-categories by difficulties, we\\nalso see that some information are more helpful for simpler or\\nharder requests versus the others. For instance, providing extra\\nsample column values (or providing them all for disambiguation) is\\nmore helpful for challenging questions, where the question is more\\nnuanced and/or ambiguous that seeing exact column values can', 'nuanced and/or ambiguous that seeing exact column values can\\nhelp pick the right column and join paths. Synthetic examples show\\nthe opposite where it helps with simple and moderate questions, but\\nhurt the challenging questions. Synthetic examples are generated\\nto illustrate common SQL features and clauses using the schema\\nelements from the target DB [28]; the examples tend to be simpler\\nand have less ambiguous mappings from the question and the', 'and have less ambiguous mappings from the question and the\\nSQL as the examples questions are generated by the same model\\ndescribing its generated SQL. The hints are also interesting in that\\nit appears to be one of the most critical ingredients for accurate\\nSQL generation and it benefits moderate questions the most. BIRD\\ndatasets contain hints as to clarify nuanced concepts (e.g., “eligible\\nfree rate for K-12”) with a correct expression (“Eligible free rate', 'free rate for K-12”) with a correct expression (“Eligible free rate\\nfor K-12 = ‘Free Meal Count (K-12)‘ / ‘Enrollment (K-12)‘”), and\\nmoderate queries often consist of more nuanced concepts that\\nrequire such clarifications.\\nOne key aspect of leveraging long context is the cost. Table 8\\nillustrates how each long context information or technique con-\\ntributes to the overall context size as the latency, which are highly\\ncorrelated (we discuss the near-linear relationship between the con-', 'correlated (we discuss the near-linear relationship between the con-\\ntext size and the latency in Section 4.8). For both context size 𝐿and\\nlatency 𝑇, we report the average plus 1-S.D. because the variance\\nof the measures are quite high given the retries and self-correction\\nmechanisms. This way, the reported measures capture the majority\\nFigure 6: Breakdown of the observed error categories\\nof the requests we evaluated. Note that 𝐿and 𝑇track accumulated', 'of the requests we evaluated. Note that 𝐿and 𝑇track accumulated\\ntokens and latency per request, until the final output is returned.\\nSelf-correction (retry) can be triggered for various reasons, includ-\\ning simple syntax errors to empty results. The pipeline will retry\\nup to 5 times with appropriate fix/modification and higher tem-\\nperature. For such recursive nature, self-correction increases the\\nlatency (and the accumulated tokens) significantly; the cost grow', 'latency (and the accumulated tokens) significantly; the cost grow\\nmore exponentially with disambiguation (oversized column value\\nexamples). The online synthetic example generation also adds to\\nthe runtime latency significantly, since the generation process in-\\nvolves a long sequence of auto-regressive decoding. If synthetic\\nexamples are generated offline and retrieved at runtime , as tested\\nin Section 4.5, then the generation latency can be reduced. It is', 'in Section 4.5, then the generation latency can be reduced. It is\\ninteresting to note that adding a few thousands tokens does not\\nincrease the latency that significantly over the entire pipeline. For\\ninstance, increase of 8816 tokens per request for “verify & retry”\\ndelayed the latency only by 1.3 normalized latency units (mean +\\n1-S.D.).\\n6\\nDISCUSSION AND LIMITATIONS\\n6.1\\nError analysis\\nTo gain a deeper understanding of the discrepancies between gen-', '6.1\\nError analysis\\nTo gain a deeper understanding of the discrepancies between gen-\\nerated SQL and ground truth SQL, we randomly sampled 20% of the\\ninstances where our baseline output deviated from the provided\\ngolden SQL in the BIRD dev. Figure 6 presents a breakdown of the\\nobserved error categories and their respective proportions within\\nthis sampled subset.\\nThe errors are categorized hierarchically. The inner ring of the\\nchart depicts three high-level classifications: \"Vague Question\" rep-', 'chart depicts three high-level classifications: \"Vague Question\" rep-\\nresenting instances where the generated SQL was broadly correct\\nbut was marked incorrect due to the vagueness of the correspond-\\ning NL question; \"Incorrect Generated SQL,\" indicating cases where\\n9', 'Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, and Fatma Özcan\\nTable 8: Ablation analysis of the long-context NL2SQL pipeline. The pipeline shows strong performance on BIRD dev, even\\nwithout any fine-tuning and/or multiple candidate generation (a.k.a., self-consistency). The context size per request also\\nincreases significantly, and we measure the total accumulated tokens over any refinement iterations (self-correction) per given', 'user request. We report the 1-standard-deviation from the mean stats to bound the majority of the requests.\\nBIRD Dev Ex Acc (%)\\nContext Size (tok/req)\\nLatency (𝑇units/req)\\nLong Context NL2SQL\\nSimple\\nModerate\\nChallenging\\nOverall\\n¯𝐿+ 𝜎\\n¯𝑇+ 𝜎\\n+ All DB table schema\\n52.22 ( - )\\n30.82 ( - )\\n32.41 ( - )\\n43.87 ( - )\\n10772.77 ( - )\\n1.0 ( - )\\n+ Hints\\n67.35 ( ↑15.13 )\\n51.08 ( ↑20.26 )\\n44.14 (↑11.73)\\n60.23 (↑16.36)\\n10796.12 (↑23.35)\\n1.0 ( - )\\n+ Sample column values\\n68.11 ( ↑0.76)\\n53.66 ( ↑2.58)', '60.23 (↑16.36)\\n10796.12 (↑23.35)\\n1.0 ( - )\\n+ Sample column values\\n68.11 ( ↑0.76)\\n53.66 ( ↑2.58)\\n49.66 ( ↑5.52 )\\n61.99 (↑1.76)\\n15568.47 ( ↑4772.35)\\n1.0 ( - )\\n+ Self-correction\\n71.03 ( ↑2.92)\\n56.25 ( ↑2.59 )\\n52.41 (↑2.75)\\n64.80 ( ↑2.81)\\n50142.14 ( ↑34573.67)\\n2.5 (↑1.5)\\n+ Disambiguation\\n71.14 ( ↑0.11)\\n57.54 ( ↑1.29 )\\n55.17 ( ↑2.76 )\\n65.51 ( ↑0.71)\\n385141.42 ( ↑334999.28)\\n33.5 (↑32)\\n+ Synthetic examples\\n72.32 ( ↑1.18)\\n59.05 ( ↑1.51 )\\n53.79 (↓1.38 )\\n66.56 (↑1.05)\\n390413.05 (↑5271.62902)', '72.32 ( ↑1.18)\\n59.05 ( ↑1.51 )\\n53.79 (↓1.38 )\\n66.56 (↑1.05)\\n390413.05 (↑5271.62902)\\n35.8 + 90.8a (↑93.1)\\n+ Verify & retryb\\n72.65 ( ↑0.33)\\n59.70 ( ↑0.65 )\\n55.86 ( ↑2.07 )\\n67.14 (↑0.58 )\\n399229.81 (↑8816.75)\\n37.1 + 90.8 (↑1.3)\\naWe measure online example generation latency separately, which can be hidden/reduced if pre-generated and retrieved instead.\\nbThe average number of attempts is 1.01, where the first output is accepted most of the time, except a few challenging cases.', 'Table 9: Long-context NL2SQL BIRD dev Ex Acc upper bounds\\n(at least one of the candidates is correct) via multiple output\\ncandidates generation.) guarantees perfect recall\\n# candidates\\n1\\n3\\n5\\n7\\n9\\nEx Acc (%)\\n65.97\\n68.84\\n68.97\\n69.69\\n70.60\\nthe generated SQL contained fundamental errors; and \"Incorrect\\nGolden SQL,\" encompassing scenarios where the provided golden\\nSQL itself is incorrect.\\nThe outer ring provides a finer-grained breakdown of the error', 'SQL itself is incorrect.\\nThe outer ring provides a finer-grained breakdown of the error\\ntypes within each high-level category. For example, within \"In-\\ncorrect Generated SQL,\" we observe subcategories such as \"Join\",\\n\"Logic\", and \"Aggregation,\" each highlighting a specific type of fail-\\nure in the generated SQL. Specifically, \"Logic\" refers to cases where\\nthe generated query failed to logically understand the intent of\\nthe user question. The distribution of errors across these subcate-', 'the user question. The distribution of errors across these subcate-\\ngories reveals that issues related to joins, filtering, and aggregation\\ncontribute significantly to the overall error rate.\\n6.2\\nFurther performance improvement via\\nself-consistency\\nSelf-consistency is another very popular technique for NL2SQL [7,\\n9, 10, 28, 32, 35], where multiple candidates are generated and the\\nmost consistent answer or the best one is selected by a fine-tuned', 'most consistent answer or the best one is selected by a fine-tuned\\npicker. As evidenced by the latest state-of-the-arts [10, 28] on\\nthe BIRD leaderboard using this technique, generating multiple\\ncandidates, with different generators (a.k.a. NL2SQL pipelines) and\\neach generator trying multiple times, and choosing the best one has\\nbecome a crucial technique for achieving high accuracy in NL2SQL.\\nOne of the state-of-the-arts, CHASE-SQL, in Table 1 uses our long', 'One of the state-of-the-arts, CHASE-SQL, in Table 1 uses our long\\ncontext pipeline as one of the three candidate generators to yield\\none of the strongest results on the BIRD leaderboard. One caveat\\nof self-consistency is that this can quickly become expensive in\\nterms of latency and the number of LLM calls. Self-consistency\\nis orthogonal to our work, and can further improve the NL2SQL\\nperformance if used together with our pipeline. Table 9 illustrates', 'performance if used together with our pipeline. Table 9 illustrates\\nhow repeatedly generating multiple output candidates from our\\nlong context pipeline can perform (generation temperature set to\\n0.5). With an oracle picker (i.e., at least one of the candidates is\\ncorrect per question) we can uplift the accuracy beyond 70%. In\\n[10, 28] the candidate sets are generated using multiple pipelines as\\ndifferent pipelines with different strategies result in more diverse', 'different pipelines with different strategies result in more diverse\\noutputs, boosting the final accuracy of self-consistency variants\\neven further.\\nSelf-consistency is orthogonal to the use of long context and\\noutside the scope of this paper. In this work we focus on the quality\\nof individual output candidate and study the performance impli-\\ncations of leveraging long context (extra contextual information),\\nwhich has not been explored for NL2SQL before.\\n6.3', 'which has not been explored for NL2SQL before.\\n6.3\\nLong context in production and limitations\\nOur study reveals (Table 8) that a good chunk of requests (roughly\\n68%) from BIRD dev can reach 400k accumulated tokens and take up\\nto 130𝑇units. This would likely be prohibitive for most production\\nservices. And the cost will be even higher with techniques like self-\\nconsistency. Long context is a great means to add more appropriate\\ninformation, but it has to be used with some care. To leverage', 'information, but it has to be used with some care. To leverage\\nlong context properly in production, one may pick and choose\\nwhich extra information and techniques are appropriate based on\\nour analysis. For instance, one can skip disambiguation and online\\nexample generation to bound the latency of the majority of requests\\nto at around <1.5 normalized seconds. The example can be generated\\noffline and in Section 4.5 we show that example selection works', 'offline and in Section 4.5 we show that example selection works\\nwell with synthetic examples. Using flash can further reduce the\\ncost, roughly -20% in latency and -94% in dollars per request given\\nthe current pricing and the observed latency.\\nOne limitation of our study, as we study the impact of long\\ncontext and extra contextual information on NL2SQL, is missing\\nbenchmark datasets that model certain enterprise use cases where\\nthere are hundreds of very wide tables with thousands of columns.', 'there are hundreds of very wide tables with thousands of columns.\\nAlso there is this aspect of less descriptive and similar table and\\ncolumn names that render schema linking much more challenging.\\nSuch use cases would be challenging in general for schema linking\\nand also exacerbates the costs of long context techniques as with\\n10', 'Is Long Context All You Need? Leveraging LLM’s Extended Context for NL2SQL\\nExperiments, Analysis and Benchmark Paper\\nincreased amounts of schema information. To this end, we tried\\nto incorporate as many challenging NL2SQL benchmark datasets\\nand evaluated how the long-context model perform with very low\\ndensity information (e.g., providing full schema details and sample\\nvalues from tens of irrelevant tables and DBs, while only a few', 'values from tens of irrelevant tables and DBs, while only a few\\ncolumns are relevant, Table 3). We observe that extra information is\\nless distracting and the long context techniques perform as good as\\nthe baselines without them. Filtering and ranking the most relevant\\ninformation should still be critical in terms of both accuracy and\\nthe costs, but unless the retrieval is very accurate, providing more\\ninformation for higher recall at the cost of lower precision is a good', 'information for higher recall at the cost of lower precision is a good\\nstrategy. We expect these insights would hold for more extreme use\\ncases, and become more evident as long context LLMs improves in\\nthe future; for instance, our long-context NL2SQL pipeline yields\\nmuch stronger results on a enterprise data warehouse benchmark\\nBEAVER than the baseline results from [5] (see Table 2).\\n7\\nCONCLUSION\\nIn this work, we explored the potential of leveraging the extended', '7\\nCONCLUSION\\nIn this work, we explored the potential of leveraging the extended\\ncontext window offered by Google’s gemini-1.5-pro for NL2SQL. Our\\nfindings demonstrate that long-context LLMs can effectively utilize\\nthe additional context, achieving strong performances on various\\nbenchmarks (Table 2), including of 67.41% on the BIRD benchmark\\nwithout fine-tuning or computationally expensive self-consistency\\ntechniques. This performance highlights the robustness of the long-', 'techniques. This performance highlights the robustness of the long-\\ncontext models in retrieving and reasoning over extensive con-\\ntextual information. Specifically, we explore the performance im-\\nplications of including entire DB schema details, user provided\\nhints, sample column values, synthetic examples for many-shot\\nICL, for accurate SQL generation. We also show that self-correction\\nand verification can improve accuracy further at the cost of in-', 'and verification can improve accuracy further at the cost of in-\\ncreased latency and accumulated tokens. Overall, we observe that\\ngemini-1.5-pro exhibits a strong retrieval capability over the ex-\\ntended context window, even with mostly irrelevant information.\\nWe also see that the long context model is more robust to or no\\nlonger “lost in the middle” [23]).\\nOur study is a unique example of how enhanced capabilities\\nof LLMs, in this case the extended large context size, impact the', 'of LLMs, in this case the extended large context size, impact the\\nway we approach the NL2SQL problem. To the contrary to prior\\nworks where we focused on squeezing the filtered information onto\\nlimited context size, we explored the potential of providing more\\nuseful information to the model. And we showed that more infor-\\nmation can be useful in NL2SQL, helping to overcome the semantic\\nissues (SQL syntactic issues for the common SQL dialect are already', 'issues (SQL syntactic issues for the common SQL dialect are already\\nwell addressed by the base model). We also investigated the cost\\nimplication of using the long context techniques and conclude that\\nit can be complementary and is more efficient with accurate schema\\nand example retrievals and with a more emphasis on recall. In fact,\\nperfect table schema retrieval would yield stronger performance\\n(Appendix A.2) narrowing down the schema linking search space', '(Appendix A.2) narrowing down the schema linking search space\\nduring generation [5, 8, 32], but it is very difficult to achieve in\\npractice (Appendix A.1).\\nIn a world where the retrieval and ranking are less than perfect,\\nproviding more information and relying on the long context models’\\nstrong retrieval capability can be a viable alternative strategy, while\\nbeing more expensive. Improving the cost efficiency of long context\\nmodel serving would be an important area of research to make long', 'model serving would be an important area of research to make long\\ncontext more practical.\\nREFERENCES\\n[1] 2024. https://www.sqlite.org/index.html\\n[2] Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Luis Rosias,\\nStephanie C.Y. Chan, Biao Zhang, Aleksandra Faust, and Hugo Larochelle. 2024.\\nMany-shot In-Context Learning. In ICML 2024 Workshop on In-Context Learning.\\nhttps://openreview.net/forum?id=goi7DFHlqS\\n[3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang,', '[3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang,\\nZhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench: A\\nbilingual, multitask benchmark for long context understanding. arXiv preprint\\narXiv:2308.14508 (2023).\\n[4] Hasan Alp Caferoğlu and Özgür Ulusoy. 2024. E-SQL: Direct Schema Linking\\nvia Question Enrichment in Text-to-SQL. arXiv preprint arXiv:2409.16751 (2024).\\n[5] Peter Baile Chen, Fabian Wenz, Yi Zhang, Moe Kayali, Nesime Tatbul, Michael', '[5] Peter Baile Chen, Fabian Wenz, Yi Zhang, Moe Kayali, Nesime Tatbul, Michael\\nCafarella, Çağatay Demiralp, and Michael Stonebraker. 2024. BEAVER: An\\nEnterprise Benchmark for Text-to-SQL. arXiv preprint arXiv:2409.02038 (2024).\\n[6] José Manuel Domínguez, Benjamín Errázuriz, and Patricio Daher. 2024. Blar-SQL:\\nFaster, Stronger, Smaller NL2SQL. arXiv preprint arXiv:2401.02997 (2024).\\n[7] Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin,', '[7] Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin,\\nDongfang Lou, et al. 2023. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint\\narXiv:2307.07306 (2023).\\n[8] Avrilia Floratou, Fotis Psallidas, Fuheng Zhao, Shaleen Deep, Gunther Hagleither,\\nWangda Tan, Joyce Cahoon, Rana Alotaibi, Jordan Henkel, Abhik Singla, Alex Van\\nGrootel, Brandon Chow, Kai Deng, Katherine Lin, Marcos Campos, K. Venkatesh\\nEmani, Vivek Pandit, Victor Shnayder, Wenjing Wang, and Carlo Curino. 2024.', 'Emani, Vivek Pandit, Victor Shnayder, Wenjing Wang, and Carlo Curino. 2024.\\nNL2SQL is a solved problem... Not!. In Conference on Innovative Data Systems\\nResearch. https://api.semanticscholar.org/CorpusID:266729311\\n[9] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and\\nJingren Zhou. 2023. Text-to-SQL Empowered by Large Language Models: A\\nBenchmark Evaluation. CoRR abs/2308.15363 (2023).\\n[10] Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi', '[10] Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi\\nLi, Wei Li, Yuntao Hong, Zhiling Luo, Jinyang Gao, Liyu Mou, and Yu Li. 2024.\\nXiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL. arXiv\\npreprint arXiv:2411.08599 (2024). https://arxiv.org/abs/2411.08599\\n[11] Google Cloud. 2024. Gemini API Pricing. https://ai.google.dev/pricing#1_5pro\\nAccessed: January 7, 2025.\\n[12] Google Cloud. 2024. Gemini in BigQuery. https://cloud.google.com/bigquery/', '[12] Google Cloud. 2024. Gemini in BigQuery. https://cloud.google.com/bigquery/\\ndocs/write-sql-gemini Accessed: October 15, 2024.\\n[13] George Katsogiannis-Meimarakis and Georgia Koutrika. 2023. A survey on deep\\nlearning approaches for text-to-SQL. VLDB J. 32, 4 (2023), 905–936.\\nhttps:\\n//doi.org/10.1007/S00778-022-00776-8\\n[14] George Katsogiannis-Meimarakis, Mike Xydas, and Georgia Koutrika. 2023. Nat-\\nural Language Interfaces for Databases with Deep Learning. Proc. VLDB Endow.', 'ural Language Interfaces for Databases with Deep Learning. Proc. VLDB Endow.\\n16, 12 (2023), 3878–3881. https://doi.org/10.14778/3611540.3611575\\n[15] Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. 2021.\\nKag-\\ngleDBQA: Realistic Evaluation of Text-to-SQL Parsers. In Proceedings of the\\n59th Annual Meeting of the Association for Computational Linguistics and the\\n11th International Joint Conference on Natural Language Processing (Volume 1:', '11th International Joint Conference on Natural Language Processing (Volume 1:\\nLong Papers). Association for Computational Linguistics, Online, 2261–2273.\\nhttps://aclanthology.org/2021.acl-long.176\\n[16] Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. 2024. Mcs-\\nsql: Leveraging multiple prompts and multiple-choice selection for text-to-sql\\ngeneration. arXiv preprint arXiv:2405.07467 (2024).\\n[17] Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan,', '[17] Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan,\\nMichael Boratko, Yi Luan, Sébastien MR Arnold, Vincent Perot, Siddharth Dalmia,\\net al. 2024. Can Long-Context Language Models Subsume Retrieval, RAG, SQL,\\nand More? arXiv preprint arXiv:2406.13121 (2024).\\n[18] Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole,\\nKai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik', 'Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik\\nDuddu, Gustavo Hernández Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusu-\\npati, Prateek Jain, Siddhartha R. Jonnalagadda, Ming-Wei Chang, and Iftekhar\\nNaim. 2024. Gecko: Versatile Text Embeddings Distilled from Large Language\\nModels. ArXiv abs/2403.20327 (2024). https://api.semanticscholar.org/CorpusID:\\n268793455\\n[19] Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. 2024. The', '268793455\\n[19] Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. 2024. The\\nDawn of Natural Language to SQL: Are We Fully Ready?\\narXiv preprint\\narXiv:2406.01265 (2024).\\n[20] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. 2023. Resdsql: Decoupling\\nschema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI\\nConference on Artificial Intelligence, Vol. 37. 13067–13075.\\n[21] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang,', '[21] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang,\\nBowen Qin, Ruiying Geng, Nan Huo, et al. 2024. Can llm already serve as a\\ndatabase interface? a big bench for large-scale database grounded text-to-sqls.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n11', 'Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, and Fatma Özcan\\n[22] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. 2024.\\nLong-context llms struggle with long in-context learning.\\narXiv preprint\\narXiv:2404.02060 (2024).\\n[23] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\\nFabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models\\nUse Long Contexts. Transactions of the Association for Computational Linguistics', 'Use Long Contexts. Transactions of the Association for Computational Linguistics\\n12 (2024), 157–173. https://doi.org/10.1162/tacl_a_00638\\n[24] Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuxin Zhang, Ju\\nFan, Guoliang Li, Nan Tang, and Yuyu Luo. 2024. A Survey of NL2SQL with\\nLarge Language Models: Where are we, and where are we going? arXiv preprint\\narXiv:2408.05109 (2024).\\n[25] Xiping Liu and Zhao Tan. 2023. Divide and prompt: Chain of thought prompting', '[25] Xiping Liu and Zhao Tan. 2023. Divide and prompt: Chain of thought prompting\\nfor text-to-sql. arXiv preprint arXiv:2304.11556 (2023).\\n[26] Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi.\\n2024. The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\\nLanguage Models. arXiv preprint arXiv:2408.07702 (2024).\\n[27] Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang,\\nArman Cohan, and Dragomir Radev. 2023.\\nEnhancing few-shot text-to-sql', 'Arman Cohan, and Dragomir Radev. 2023.\\nEnhancing few-shot text-to-sql\\ncapabilities of large language models: A study on prompt design strategies. arXiv\\npreprint arXiv:2305.12586 (2023).\\n[28] Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan Talaei,\\nGaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Özcan, and Sercan Ö Arık.\\n2024. CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate\\nSelection in Text-to-SQL. arXiv preprint arXiv:2410.01943 (2024).', 'Selection in Text-to-SQL. arXiv preprint arXiv:2410.01943 (2024).\\n[29] Mohammadreza Pourreza and Davood Rafiei. 2024. Din-sql: Decomposed in-\\ncontext learning of text-to-sql with self-correction. Advances in Neural Informa-\\ntion Processing Systems 36 (2024).\\n[30] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy\\nLillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat,\\nJulian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding', 'Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding\\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024).\\n[31] Chang-You Tai, Ziru Chen, Tianshu Zhang, Xiang Deng, and Huan Sun. 2023.\\nExploring chain-of-thought style prompting for text-to-sql.\\narXiv preprint\\narXiv:2305.14215 (2023).\\n[32] Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, and\\nAmin Saberi. 2024. CHESS: Contextual Harnessing for Efficient SQL Synthesis.', 'Amin Saberi. 2024. CHESS: Contextual Harnessing for Efficient SQL Synthesis.\\narXiv preprint arXiv:2405.16755 (2024).\\n[33] Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng Chai,\\nZhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, and Zhoujun Li. 2024. MAC-SQL: A\\nMulti-Agent Collaborative Framework for Text-to-SQL. arXiv:cs.CL/2312.11242\\n[34] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew', '[34] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew\\nRichardson. 2020. RAT-SQL: Relation-Aware Schema Encoding and Linking for\\nText-to-SQL Parsers. In Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel\\nTetreault (Eds.). Association for Computational Linguistics, Online, 7567–7578.\\nhttps://doi.org/10.18653/v1/2020.acl-main.677', 'https://doi.org/10.18653/v1/2020.acl-main.677\\n[35] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang,\\nAakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves\\nChain of Thought Reasoning in Language Models. In The Eleventh International\\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\\nOpenReview.net. https://openreview.net/forum?id=1PL1NIMMrw\\n[36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,', '[36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reason-\\ning in large language models. Advances in neural information processing systems\\n35 (2022), 24824–24837.\\n[37] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020.\\nTaBERT: Pretraining for joint understanding of textual and tabular data. arXiv\\npreprint arXiv:2005.08314 (2020).', 'preprint arXiv:2005.08314 (2020).\\n[38] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li,\\nJames Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\\nRadev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and\\nCross-Domain Semantic Parsing and Text-to-SQL Task. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing. Association for\\nComputational Linguistics, Brussels, Belgium.', 'Computational Linguistics, Brussels, Belgium.\\n[39] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan,\\nYongji Wang, and Jian-Guang Lou. 2022. Large language models meet nl2code:\\nA survey. arXiv preprint arXiv:2212.09420 (2022).\\n[40] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2024. Judging LLM-as-a-judge with MT-', 'Joseph E. Gonzalez, and Ion Stoica. 2024. Judging LLM-as-a-judge with MT-\\nbench and Chatbot Arena. In Proceedings of the 37th International Conference on\\nNeural Information Processing Systems (NIPS ’23). Curran Associates Inc., Red\\nHook, NY, USA, Article 2020, 29 pages.\\n[41] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating\\nstructured queries from natural language using reinforcement learning. arXiv\\npreprint arXiv:1709.00103 (2017).', 'preprint arXiv:1709.00103 (2017).\\n[42] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang,\\nDale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-\\nmost prompting enables complex reasoning in large language models. arXiv\\npreprint arXiv:2205.10625 (2022).\\nA\\nAPPENDIX\\nA.1\\nTable retrieval simulation\\nFor a realistic table retrieval simulation, we ran the BIRD bench\\nagainst BigQuery’s SQL Code Assist feature[12]. The result differs', 'against BigQuery’s SQL Code Assist feature[12]. The result differs\\nfrom the standard BIRD bench setup, as it mirrors the challenges\\nof serving production users of BigQuery. Conventionally when\\nrunning the BIRD bench, each query is linked to a single database\\ncontaining up to 13 tables. BigQuery operates differently; users\\nmay ask questions about any table that they have access to, and\\nBigQuery’s retrieval system uses user interaction histories to re-', 'BigQuery’s retrieval system uses user interaction histories to re-\\nduce the search space. For realistic results the user interactions\\nwere seeded to match the distribution of queries in BigQuery’s\\nproduction traffic, with a bias towards session-starting queries (less\\nrepeated queries), as these present a more difficult retrieval problem.\\nFor each example in the benchmark, table retrieval operates as\\nfollows: 1) The simulated user view and query interactions narrow', 'follows: 1) The simulated user view and query interactions narrow\\nthe search space down to 1-100 candidates; 2) The candidates are\\nembedded and re-ranked with Gecko [18] for relevance to the user\\nquery; 3) A fixed top-k candidates are passed on to the NL2SQL\\nstage.\\nAt the time of test, the recall of the first stage was 82%. As such,\\n82% is the maximum attainable end-to-end recall, and represents a\\nnear perfect result from the re-ranking stage.\\nA.2\\nImpact of good column selection', 'near perfect result from the re-ranking stage.\\nA.2\\nImpact of good column selection\\nTable 10: Relevant schema element filtering (TBR: Table Re-\\ntrieval, CR: Column Retrieval) and Ex Acc on BIRD dev\\nFiltered Schema\\nGround Truth Schema\\nEx Acc (%)\\n64.08\\n72.43\\nTBR Recall (%)\\n97.69\\n100\\nTBR Precision(%)\\n89.72\\n100\\nCR Recall(%)\\n97.12\\n100\\nCR Precision(%)\\n69.43\\n100\\nWe evaluate the impact of schema selection on execution accu-\\nracy using the BIRD dev. We compare a filtered schema against', 'racy using the BIRD dev. We compare a filtered schema against\\na perfect ground truth schema. The filtered schema, constructed\\nsimilarly to CHESS [32], incorporates relevant tables and columns,\\ntheir descriptions, and relevant example values. 10 reports execu-\\ntion accuracy (Ex Acc), as well as table retrieval (TBR) and column\\nretrieval (CR) recall and precision. While the ground truth schema\\nachieves perfect performance (100%) on all metrics, the filtered', 'achieves perfect performance (100%) on all metrics, the filtered\\nschema demonstrates high recall, correctly identifying most rel-\\nevant schema elements (97.69% table retrieval recall and 97.12%\\ncolumn retrieval recall). However, precision is lower (89.72% for\\ntable retrieval and 69.43% for column retrieval), indicating the inclu-\\nsion of some irrelevant columns/tables. It is important to note that\\nthe schema filtering algorithm incurs substantial cost, requiring\\n12', 'Is Long Context All You Need? Leveraging LLM’s Extended Context for NL2SQL\\nExperiments, Analysis and Benchmark Paper\\nYou are a SQLite SQL expert.\\nYou need to generate SQLite SQL query given a question in natural lan-\\nguage. The database (\"{db_name}\") structure is defined by the following table schemas\\n(comments after ’–’ provide additional column descriptions).\\nGiven the \"Table creation statements\" and the \"Question\", you need understand the\\ndatabase and columns.', 'database and columns.\\nConsider the natural language question to SQL query \"Examples\".\\nAlso consider some useful \"Hints\" if provided.\\n###Table creation statements###\\n{schema}\\n***************************\\n###Examples###\\n{examples}\\n***************************\\n###Documentation###\\n{documentation}\\n***************************\\n###Question###\\n{question}\\n(Hints: {hints})\\n***************************\\nNow generate SQLite SQL query to answer the given \"Question\".\\nOutput the SQL query string ONLY.', 'Now generate SQLite SQL query to answer the given \"Question\".\\nOutput the SQL query string ONLY.\\nFigure 8: Prompt template for NL2SQL generation.\\nYou are a SQLite SQL expert.\\nSomeone had a question and they tried to run a SQL query to fetch the data for it.\\nHowever, the query execution failed for some error. Now you need to fix the query\\nbased on the previous execution error.\\nThe database structure is defined by the following table schemas (comments after ’–’', 'The database structure is defined by the following table schemas (comments after ’–’\\nprovide additional column descriptions).\\n**************************\\n###Table creation statements###\\n{schema}\\n**************************\\nThe original question is:\\n{question}\\nThe SQL query executed was:\\n{sql}\\nThe execution failed with error:\\n{error}\\n**************************\\nBased on the question, table schemas and the errored query, analyze what the query\\nwas trying to achieve and fix the error.', 'was trying to achieve and fix the error.\\nIf the error cannot be fixed by fixing the query, for example, connection error or\\npermission error, just output the original query. Otherwise, think step by step about\\ngenerating correct SQLite SQL result!\\nAnalyze the error and how to fix.\\nDONT FORGET Additional rules to generate correct SQLite SQL dialect:\\n{rules}\\nWhen you are OK with the fixed query, output the sqlite query string ONLY. It should', '{rules}\\nWhen you are OK with the fixed query, output the sqlite query string ONLY. It should\\nbe the query in plain text. Your answer must be a single SQL statement.\\nFigure 9: Prompt template for fix & rewrite.\\nYou are a SQLite SQL expert.\\nYour job is to verify the correctness of a given SQL query both syntactically and\\nsemantically. That means you would have to ensure the SQL query is syntatically\\ncorrect, and also it does return what the user is asking for in the natural language\\n\"Question.\"', 'correct, and also it does return what the user is asking for in the natural language\\n\"Question.\"\\nGiven the \"Table creation statements\" and the \"Question and Hints\", you need\\nunderstand the database and the relevant table columns. The database structure is\\ndefined by the following table schemas (comments after ’–’ provide additional column\\ndescriptions).\\nUse all these information to identify the correct literals, table column names and also\\nthe correct join path.', 'the correct join path.\\nAlso pay close attention to the \"Hints\" as they contain very important information to\\nanswer the question correctly.\\n***************************\\n###SQL to verify###\\n{sql}\\n***************************\\n###Question and Hints###\\n{question}\\n***************************\\n###Table Creation Statements###\\n{schema}\\n***************************\\nRemember, following hints very closely is the key to the correct answer!!!', 'Remember, following hints very closely is the key to the correct answer!!!\\nIf you think the SQL query is incorrect, then return an empty string \"\". If you are\\nconfident that the SQL query is correct, return it as-is.\\nFigure 10: Prompt template for output verification.\\nan average of 78 LLM calls and 339,965 tokens per request on the\\nBIRD dev.\\nA.3\\nSQLite document retrieval\\nHere we show SQLite documentation retrieval results for example', 'A.3\\nSQLite document retrieval\\nHere we show SQLite documentation retrieval results for example\\nquestions from BIRD dev dataset. We employed two chunking strat-\\negy to split the entire document by files and sections, respectively.\\nWe implemented a simple retrieval pipeline based on question &\\ndocument similarity. The retrieval is challenging as the question\\ncontains limited information as to what documentation sections\\nare relevant, and writing a SQL query can draw from many sections', 'are relevant, and writing a SQL query can draw from many sections\\nfrom the documentations. Even with the long context, we can only\\nfit a select number of document chunks. We can summarize and\\ntry to compress to more chunks, but it will not be enough to fit the\\nentire SQLite documentation or even all the relevant contents.\\nAs shown in Table 6, having a large text from the documentation\\ndoes not seem to affect the final generation quality much. Another', 'does not seem to affect the final generation quality much. Another\\ninteresting aspect of citing the SQLite documentation for NL2SQL\\nis that the examples used in the documentation are often simple\\nand generic as supposed to being specific to the user question and\\ndata. This is expected as the documentation tries to be illustrative\\nof the general concepts and for the general audience.\\n13', 'Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, and Fatma Özcan\\nQuestion: Among the cards with converted mana cost higher than 5 in the set\\nColdsnap, how many of them have unknown power?\\nSELECT SUM(CASE WHEN T1.power LIKE ’*’ OR T1.power IS NULL THEN\\n1 ELSE 0 END) FROM cards AS T1 INNER JOIN sets AS T2 ON T2.code = T1.setCode\\nWHERE T2.name = ’Coldsnap’ AND T1.convertedManaCost > 5\\nRetrieved coarse-grained chunks: Status Parameters for prepared statements,', 'Retrieved coarse-grained chunks: Status Parameters for prepared statements,\\nBuilt-In Scalar SQL Functions, List Of SQLite Functions\\nRetrieved fine-grained chunks: Rowid Tables, Definitions, Additional Notes, Searching,\\nStatus Parameters for prepared statements, Intellectual Property, Example Uses Of\\nRow Values, List Of Core Functions, Release Notes, signed-number\\nFigure 7: Chunked SQLite documentation chunks retrieval\\nresults (relevant chunks, irrelevant chunks) with BIRD dev\\nquestions.\\nA.4', 'results (relevant chunks, irrelevant chunks) with BIRD dev\\nquestions.\\nA.4\\nPrompt templates\\nThe following are the generation, correction and verification prompt\\ntemplates.\\n14']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def limpia_caracteres(filename):\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '', filename)\n",
    " \n",
    "def prueba_chunks(papers):\n",
    "    \"\"\"\n",
    "    Recibe una lista de papers de arXiv y almacena sus embeddings en una base de datos vectorial de ChromaDB.\n",
    "    \n",
    "    Parameters:\n",
    "    - papers (list of dict): Lista de documentos con 'title', 'authors', 'chapters' (diccionario con capítulos y su contenido).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configurar el text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \" \", \".\"]\n",
    "    )\n",
    "\n",
    "    pdfs = []\n",
    "    for paper in papers:\n",
    "        titulo = limpia_caracteres(paper.title)\n",
    "        os.makedirs(\"pdfs\", exist_ok=True)\n",
    "        # Download the PDF to the PWD with a custom filename.\n",
    "        paper.download_pdf(filename=\"pdfs/\" + titulo + \".pdf\")\n",
    "        # Cargar el texto del paper\n",
    "        pdf_loader = PyMuPDFLoader(\"pdfs/\" + titulo + \".pdf\")\n",
    "        pdfs.append(pdf_loader.load())\n",
    "    \n",
    "    docs = []\n",
    "    for pdf in pdfs:\n",
    "        # juntamos todas las páginas en un solo texto, separado por dos saltos de línea. (pdf es una lista de paginas)\n",
    "        full_text = \"\\n\\n\".join([page.page_content for page in pdf])\n",
    "        docs.append(full_text)\n",
    "        \n",
    "    chunks =[]\n",
    "    for full_text in docs:\n",
    "        chunks.append(text_splitter.split_text(full_text))\n",
    "        \n",
    "    print(len(chunks[0]), chunks[0])\n",
    "    # Eliminamos el contenido de la carpeta pdfs\n",
    "    # shutil.rmtree(\"pdfs\")\n",
    "    # Vaciar el contenido de la carpeta pdfs\n",
    "    for filename in os.listdir(\"pdfs\"):\n",
    "        file_path = os.path.join(\"pdfs\", filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "search = arxiv.Search(\n",
    "        query = \"LLM\",\n",
    "        max_results = 3,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "       \n",
    "papers = arxiv_client.results(search)\n",
    "prueba_chunks(papers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Parámetros\n",
    "MAX_TOKENS = 1000\n",
    "TEMPERATURE = 0.3\n",
    "MODEL= \"gpt-3.5-turbo\"\n",
    "\n",
    "# OpenAI API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "# Inicializamos el modelo de openai que vamos a utilizar con langchain.\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,  \n",
    "    temperature=TEMPERATURE,        \n",
    "    max_tokens=MAX_TOKENS,        \n",
    "    openai_api_key=openai_api_key,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Realizamos una prueba de inferencia\n",
    "input_text = \"What is the meaning of life?\"\n",
    "\n",
    "\n",
    "# Inicializamos también el modelos de embeddings seleccionado.\n",
    "# embeddings = OpenAIEmbeddings(    # (openai embeddings)\n",
    "#     model=EMBEDDING_MODEL,  \n",
    "#     openai_api_key=openai_api_key    \n",
    "# )\n",
    "\n",
    "#  alternativa para obtener embeddings\n",
    "embedding_model = CohereEmbeddings(\n",
    "                model=\"embed-english-light-v3.0\",\n",
    "                cohere_api_key=cohere_api_key\n",
    "            )\n",
    "\n",
    "# Iinicializamos el cliente de arXiv\n",
    "arxiv_client = arxiv.Client()\n",
    "\n",
    "# Se inicializa ChromaDB para almacenamiento vectorial persistente\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "vector_collection = chroma_client.get_or_create_collection(\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función que permita monitorizar el coste de la llamada a la API de OpenAI.\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def llamada_monitorizada(llm, texto):\n",
    "    \"\"\" Función que realiza una llamada al modelo de lenguaje especificado y monitoriza el coste de la llamada.\n",
    "    Args:\n",
    "        llm (OpenAI): Modelo de lenguaje\n",
    "        texto (str): Texto de entrada\n",
    "\n",
    "    Returns:\n",
    "        response (str): Texto de salida generado por el modelo de lenguaje\n",
    "    \"\"\"\n",
    "    \n",
    "    with get_openai_callback() as callback:\n",
    "        response = llm.invoke(texto)\n",
    "        \n",
    "        # Detalles de uso y coste de la llamada.\n",
    "        print(f\"Tokens usados: {callback.total_tokens}\")\n",
    "        print(f\"Coste de la llamada: ${callback.total_cost:.5f}\")\n",
    "        \n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens usados: 40\n",
      "Coste de la llamada: $0.00003\n",
      "content=\"J'adore la programmation.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 31, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-569d1842-2412-41b3-b270-89ac98b2dda8-0' usage_metadata={'input_tokens': 31, 'output_tokens': 9, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "# Prueba de llamada monitorizada.\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "response = llamada_monitorizada(llm, messages)\n",
    "print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaración de herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def buscar_publicaciones_arxiv(consulta: str, max_resultados: int = 3) -> object:\n",
    "    \"\"\"\n",
    "    Busca publicaciones científicas en arXiv según una consulta del usuario.\n",
    "    \n",
    "    Args:\n",
    "        consulta (str): El término de búsqueda para arXiv.\n",
    "        max_resultados (int): Número máximo de publicaciones a devolver.\n",
    "\n",
    "    Returns:\n",
    "        PDFs de las publicaciones encontradas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Iinicializamos el cliente de arXiv\n",
    "        arxiv_client = arxiv.Client()\n",
    "\n",
    "        # Realizar la consulta a arXiv\n",
    "        search = arxiv.Search(\n",
    "            query=consulta,\n",
    "            max_results=max_resultados,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        \n",
    "        results = arxiv_client.results(search)\n",
    "        \n",
    "        if not results:\n",
    "            return \"No se encontraron publicaciones.\"\n",
    "        \n",
    "        for paper in results:\n",
    "            print(paper.title)\n",
    "            \n",
    "        return results\n",
    "          \n",
    "    except Exception as e:\n",
    "        return f\"Error al buscar en arXiv: {str(e)}\"\n",
    "\n",
    "\n",
    "def store_papers_in_chroma(papers, emb_model, collection):\n",
    "\n",
    "    # Configurar el text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    for paper in papers:\n",
    "        title = paper.get(\"title\", \"Unknown Title\")\n",
    "        authors = \", \".join(paper.get(\"authors\", []))\n",
    "        chapters = paper.get(\"chapters\", {})  # Ahora 'chapters' es un diccionario con capítulos y contenido\n",
    "        \n",
    "        for chapter, content in chapters.items():\n",
    "            # Crear el contenido del capítulo\n",
    "            full_text = f\"Title: {title}\\nAuthors: {authors}\\nChapter: {chapter}\\n\\n{content}\"\n",
    "            \n",
    "            # Dividir en chunks\n",
    "            chunks = text_splitter.split_text(full_text)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{title.replace(' ', '_')}_{chapter.replace(' ', '_')}_{i}\"\n",
    "                embedding = emb_model.embed_query(chunk)\n",
    "                \n",
    "                # Almacenar en ChromaDB\n",
    "                collection.add(\n",
    "                    ids=[chunk_id],\n",
    "                    embeddings=[embedding],\n",
    "                    metadatas=[{\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"chapter\": chapter,\n",
    "                        \"chunk_index\": i\n",
    "                    }],\n",
    "                    documents=[chunk]\n",
    "                )\n",
    "    \n",
    "    print(f\"Stored {len(papers)} papers in ChromaDB collection '{collection}'.\")\n",
    "\n",
    "\n",
    "def retrieve_similar_chunks(query, collection, emb_model, n_results=5):\n",
    "    \"\"\"\n",
    "    Realiza RAG sobre la base de datos de ChromaDB buscando los n chunks más similares al texto de entrada.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): Texto de búsqueda.\n",
    "    - collection (str): Nombre de la colección en ChromaDB.\n",
    "    - n_results (int): Número de resultados más similares a devolver.\n",
    "    -\n",
    "    \n",
    "    Returns:\n",
    "    - List[dict]: Lista con los chunks más relevantes y sus metadatos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generar embedding de la consulta\n",
    "        query_embedding = emb_model.embed_query(query)\n",
    "        \n",
    "        # Buscar en ChromaDB\n",
    "        results = collection.query(query_embeddings=[query_embedding], n_results=n_results)\n",
    "        \n",
    "        # Extraer documentos y metadatos\n",
    "        retrieved_chunks = []\n",
    "        for i in range(len(results[\"ids\"][0])):\n",
    "            retrieved_chunks.append({\n",
    "                \"chunk\": results[\"documents\"][0][i],\n",
    "                \"title\": results[\"metadatas\"][0][i][\"title\"],\n",
    "                \"authors\": results[\"metadatas\"][0][i][\"authors\"],\n",
    "                \"chapter\": results[\"metadatas\"][0][i][\"chapter\"]\n",
    "            })\n",
    "        \n",
    "        return retrieved_chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener información de la bbdd: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "def publicar_en_x(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Publica un texto en X (Twitter) utilizando la API de Tweepy.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): El texto que se desea publicar en X.\n",
    "    Returns:\n",
    "        str: Mensaje de éxito o error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        twitter_api_key = os.getenv(\"TWITTER_API_KEY\")\n",
    "        twitter_api_secret = os.getenv(\"TWITTER_API_SECRET\")\n",
    "        twitter_access_token = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "        twitter_access_token_secret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "        bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "        client_id = os.getenv(\"CLIENT_ID\")\n",
    "        client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "        # v2.0\n",
    "        auth = tweepy.OAuth2UserHandler(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, scope=\"tweet.write\")\n",
    "        print(auth)\n",
    "\n",
    "        client_X = tweepy.Client(\n",
    "                consumer_key=twitter_api_key,\n",
    "                consumer_secret=twitter_api_secret,\n",
    "                access_token=twitter_access_token,\n",
    "                access_token_secret=twitter_access_token_secret,\n",
    "            )\n",
    "\n",
    "        response = client_X.create_tweet(text=texto, user_auth=True)\n",
    "        print(f\"Tweet publicado exitosamente: {response.data}\")\n",
    "        \n",
    "        return response.status_code #Añadir enlace al perfil de x\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error al publicar el tweet: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "\n",
    "\n",
    "class SearchOnArxivTool(BaseTool):\n",
    "    \"\"\"Herramienta de búsqueda de artículos científicos en arXiv.\"\"\"\n",
    "\n",
    "    name: str = \"SearchOnArxivTool\"\n",
    "    description: str = \"Searches for scientific articles on arXiv.\"\n",
    "    \n",
    "    def _run(self, consulta: str, max_resultados: int = 3) -> str:\n",
    "        \"\"\"Synchronous logic for executing the tool.\"\"\"\n",
    "        try:\n",
    "            return buscar_publicaciones_arxiv(consulta, max_resultados)\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    async def _arun(self, consulta: str, max_resultados: int = 3) -> str:\n",
    "        \"\"\"Asynchronous logic for executing the tool.\"\"\"\n",
    "        raise NotImplementedError(\"This tool does not support asynchronous operations.\")\n",
    "\n",
    "\n",
    "class RetrieveTool(BaseTool):\n",
    "    \"\"\"Herramienta para recuperar chunks similares en ChromaDB.\"\"\"\n",
    "\n",
    "    name: str = \"RetrieveTool\"\n",
    "    description: str = \"Retrieves similar chunks from ChromaDB.\"\n",
    "    \n",
    "    def _run(self, query: str, n_results: int = 5) -> str:\n",
    "        \"\"\"Synchronous logic for executing the tool.\"\"\"\n",
    "        try:\n",
    "            return retrieve_similar_chunks(query, vector_collection, embedding_model, n_results)\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    async def _arun(self, query: str, collection: str, emb_model: str, n_results: int = 5) -> str:\n",
    "        \"\"\"Asynchronous logic for executing the tool.\"\"\"\n",
    "        raise NotImplementedError(\"This tool does not support asynchronous operations.\")\n",
    "\n",
    "\n",
    "class TwitterPostTool(BaseTool):\n",
    "    \"\"\"Herramienta para publicar texto en X(Twitter).\"\"\"\n",
    "    \n",
    "    name: str = \"TwitterPostTool\"\n",
    "    description: str = (\n",
    "        \"Posts a message on X(Twitter). \"\n",
    "        \"Use this to share a message given a in your X account.\"\n",
    "    )\n",
    "    \n",
    "    def _run(self, texto: str) -> str:\n",
    "        \"\"\"Synchronous logic for executing the tool.\"\"\"\n",
    "        try:\n",
    "            return publicar_en_x(texto)\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "    async def _arun(self, texto: str) -> str:\n",
    "        \"\"\"Asynchronous logic for executing the tool.\"\"\"\n",
    "        raise NotImplementedError(\"This tool does not support asynchronous operations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario con las herramientas disponibles.\n",
    "herramientas_disponibles = {\n",
    "    \"arxiv_search\": SearchOnArxivTool(),\n",
    "    \"rag\": RetrieveTool(),\n",
    "    \"twitter_publisher\": TwitterPostTool()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de los nodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Literal, Sequence\n",
    "from langchain.schema import BaseMessage, HumanMessage\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Clase para el nodo que enruta las solicitudes a las herramientas correspondientes.\"\"\"\n",
    "    \n",
    "    next: Literal[\"SearchOnArxivTool\", \"RetrivalDocumentInformation\" \"TwitterPostTool\", \"FINISH\"]\n",
    "    \n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Clase para el estado del agente.\"\"\"\n",
    "    \n",
    "    messages: Sequence[BaseMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Literal, Sequence\n",
    "from langchain.schema import BaseMessage, HumanMessage\n",
    "from langchain.tools import Tool\n",
    "from langgraph.graph import StateGraph, MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "# Función para crear agentes genérica (Subgrafo que solo permite el uso de una herramienta por agente).\n",
    "def create_agent(llm, tool):\n",
    "    llm_with_tools = llm.bind_tools([tool])\n",
    "    def agent_function(state: AgentState):\n",
    "        return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "    \n",
    "    graph_builder = StateGraph(AgentState)\n",
    "    graph_builder.add_node(\"agent\", agent_function)\n",
    "    graph_builder.set_entry_point(\"agent\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "system_prompt = \"\"\n",
    "\n",
    "# Create supervisor node function\n",
    "def supervisor_node(state: MessagesState) -> Command[Literal[\"arxiv_search\", \"rag\", \"twitter_publisher\", \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "    goto = response[\"next\"]\n",
    "    print(f\"Next Worker: {goto}\")\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "    return Command(goto=goto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo Multiagente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "# Declaración de los elementos del grafo.\n",
    "# Agentes de las herramientas.\n",
    "search_on_arxiv_agent = create_agent(llm, herramientas_disponibles[\"arxiv_search\"])\n",
    "retrival_document_information_agent= create_agent(llm, herramientas_disponibles[\"rag\"])\n",
    "twitter_post_agent = create_agent(llm, herramientas_disponibles[\"twitter_publisher\"])\n",
    "\n",
    "# Nodos de las herramientas.\n",
    "def search_on_arxiv_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = search_on_arxiv_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"SearchOnArxivTool\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\"\n",
    "    )\n",
    "\n",
    "def retrival_document_information_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = retrival_document_information_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"RetrivalDocumentInformation\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\"\n",
    "    )\n",
    "\n",
    "def twitter_post_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = twitter_post_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"TwitterPostTool\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Generación del grafo.\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Añadimos los nodos de las herramientas al grafo.\n",
    "builder.add_edge(START,\"supervisor\")\n",
    "\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"arxiv_search\", search_on_arxiv_node)\n",
    "builder.add_node(\"rag\", retrival_document_information_node)\n",
    "builder.add_node(\"twitter_publisher\", twitter_post_node)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización del Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Error al mostrar la imagen: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se incializa el cliente y se crea una BBDD (collection)\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Crea la colección\n",
    "collection = chroma_client.create_collection(name=\"arxiv\")\n",
    "\n",
    "# Añade documentos a la colección\n",
    "collection.add(\n",
    "    documents=[\"ejemplo número 1\", \"ejemplo número 2\", \"ejemplo número 3\"],\n",
    "    metadatas=[{\"source\": \"arxiv\"}, {\"source\": \"arxiv\"}, {\"source\": \"arxiv\"}],\n",
    "    ids=[\"1\", \"2\", \"3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Front "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adria\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def yes_man(message, history):\n",
    "    if message.endswith(\"?\"):\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"Ask me anything!\"\n",
    "    \n",
    "def user_interface(user_input, history=[]):\n",
    "    state= {\"messages\": history + [HumanMessage(content=user_input)]}\n",
    "    results = graph.invoke(state)\n",
    "    response = results[\"messages\"][-1].content\n",
    "    history.append((user_input, response))\n",
    "    return response, history\n",
    "   \n",
    "\n",
    "demo = gradio.ChatInterface(\n",
    "    fn=yes_man,\n",
    "    title=\"MULTIAGENTE ARXIV RAG\",\n",
    "    description=\"Say hello to someone!\",\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
