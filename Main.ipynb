{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adrián Gayo Andrés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado\n",
    "Se propone la creación de un prototipo con varios\n",
    "agentes que permita extraer publicaciones científicas de ArXiv y las publique en X.\n",
    "En el ITCL se busca desarrollar una herramienta basada en un sistema de agentes cuyo\n",
    "objetivo principal es permitir que los usuarios accedan a ArXiv a través de una API,\n",
    "utilizando una interfaz de chat que funcione mediante lenguaje natural. Además, el sistema\n",
    "debe ofrecer la posibilidad de que los usuarios publiquen, de forma sencilla y en cualquier\n",
    "momento, los contenidos de una o varias investigaciones en la plataforma X. Este sistema\n",
    "estará diseñado para ser semiautónomo, pero siempre bajo la supervisión directa del\n",
    "usuario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.6.3)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (2.10.5)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/codespace/.local/lib/python3.12/site-packages (from chromadb) (2.2.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (3.9.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (1.29.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (1.69.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/codespace/.local/lib/python3.12/site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/codespace/.local/lib/python3.12/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in /home/codespace/.local/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/python/3.12.1/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.41.3)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.7.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/codespace/.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/codespace/.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/codespace/.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/codespace/.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/python/3.12.1/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/python/3.12.1/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.1.21)\n",
      "Requirement already satisfied: protobuf in /usr/local/python/3.12.1/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.50b0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.27.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.2.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install arxiv\n",
    "# %pip install langchain\n",
    "# %pip install langgraph\n",
    "# %pip install chromadb --upgrade\n",
    "# %pip install cohere\n",
    "# %pip install tweepy \n",
    "# %pip install gradio\n",
    "# %pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sqlite3' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(langchain\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/sqlite3/__init__.py:70\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     67\u001b[0m     warn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and will be removed in Python 3.14\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m          \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_deprecated_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sqlite3' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import sqlite3\n",
    "import langchain\n",
    "import langgraph\n",
    "import tweepy\n",
    "import openai \n",
    "import chromadb\n",
    "import cohere\n",
    "import gradio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXiV API prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quantum algorithm for Khovanov homology\n",
      "Mukkamala-Pereñiguez master function for even-parity perturbations of the Schwarzschild spacetime\n",
      "On the distinguishability of geometrically uniform quantum states\n",
      "Stochastic Calculus and Hochschild Homology\n",
      "Measured Hockey-Stick Divergence and its Applications to Quantum Pufferfish Privacy\n",
      "Ensemble control of n-level quantum systems with a scalar control\n",
      "Boundary Curvature Scalars on Conformally Compact Manifolds\n",
      "A dagger kernel category of complete orthomodular lattices\n",
      "Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration\n",
      "Decoherence of Schrödinger cat states in light of wave/particle duality\n"
     ]
    }
   ],
   "source": [
    "# Iinicializamos el cliente de arXiv\n",
    "arxiv_client = arxiv.Client()\n",
    "\n",
    "# Query de ejemplo con la palabra \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "  query = \"quantum\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "# Obtenemos los resultados de la búsqueda\n",
    "results = arxiv_client.results(search)\n",
    "\n",
    "# Mostremos los resultados.\n",
    "for r in arxiv_client.results(search):\n",
    "  print(r.title)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pdfs/Coming full circle -- A unified framework for Kochen-Specker contextuality.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# También podemos descargar el PDF de un artículo.\n",
    "paper = next(arxiv_client.results(search))\n",
    "# Creamos un directorio para almacenar los PDFs en caso de que no exista.\n",
    "os.makedirs(\"pdfs\", exist_ok=True)\n",
    "# Download the PDF to the PWD with a custom filename.\n",
    "paper.download_pdf(filename=\"pdfs/\" + paper.title + \".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X API prueba (tweepy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15.0\n",
      "<tweepy.auth.OAuth2UserHandler object at 0x7911d1e5fad0>\n",
      "Tweet publicado exitosamente: {'edit_history_tweet_ids': ['1882003139789770908'], 'id': '1882003139789770908', 'text': 'Hola mundo!'}\n"
     ]
    }
   ],
   "source": [
    "# Nos autenticamos en la API de Twitter\n",
    "load_dotenv()\n",
    "twitter_api_key = os.getenv(\"TWITTER_API_KEY\")\n",
    "twitter_api_secret = os.getenv(\"TWITTER_API_SECRET\")\n",
    "twitter_access_token = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "twitter_access_token_secret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "client_id = os.getenv(\"CLIENT_ID\")\n",
    "client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "redirect_uri = \"http://127.0.0.1:5000/callback\" \n",
    "print(tweepy.__version__)\n",
    "\n",
    "# v1.0\n",
    "# auth = tw.OAuth1UserHandler(twitter_api_key, twitter_api_secret, twitter_access_token, twitter_access_token_secret)\n",
    "# api = tw.API(auth, wait_on_rate_limit=True)\n",
    "# api.verify_credentials()\n",
    "# print(\"Autenticación exitosa\")\n",
    "# api.update_status(status=\"Hello, world!\")\n",
    "# print(\"Tweet publicado exitosamente\")\n",
    "\n",
    "# v2.0\n",
    "auth = tweepy.OAuth2UserHandler(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, scope=\"tweet.write\")\n",
    "print(auth)\n",
    "\n",
    "client_X = tweepy.Client(\n",
    "        consumer_key=twitter_api_key,\n",
    "        consumer_secret=twitter_api_secret,\n",
    "        access_token=twitter_access_token,\n",
    "        access_token_secret=twitter_access_token_secret,\n",
    "    )\n",
    "\n",
    "response = client_X.create_tweet(text=\"Hola mundo!\", user_auth=True)\n",
    "print(f\"Tweet publicado exitosamente: {response.data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_chunks(papers):\n",
    "    \"\"\"\n",
    "    Recibe una lista de papers de arXiv y almacena sus embeddings en una base de datos vectorial de ChromaDB.\n",
    "    \n",
    "    Parameters:\n",
    "    - papers (list of dict): Lista de documentos con 'title', 'authors', 'chapters' (diccionario con capítulos y su contenido).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configurar el text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    for paper in papers:\n",
    "        title = paper.get(\"title\", \"Unknown Title\")\n",
    "        authors = \", \".join(paper.get(\"authors\", []))\n",
    "        chapters = paper.get(\"chapters\", {})  # Ahora 'chapters' es un diccionario con capítulos y contenido\n",
    "        \n",
    "        for chapter, content in chapters.items():\n",
    "            # Crear el contenido del capítulo\n",
    "            full_text = f\"Title: {title}\\nAuthors: {authors}\\nChapter: {chapter}\\n\\n{content}\"\n",
    "            \n",
    "            # Dividir en chunks\n",
    "            chunks = text_splitter.split_text(full_text)\n",
    "            print(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-1234567890abcdef1234567890abcdef'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Parámetros\n",
    "MAX_TOKENS = 1000\n",
    "TEMPERATURE = 0.3\n",
    "MODEL= \"gpt-3.5-turbo\"\n",
    "EMBEDDING_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# OpenAI API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Inicializamos el modelo de openai que vamos a utilizar con langchain.\n",
    "llm = OpenAI(\n",
    "    model=MODEL,  \n",
    "    temperature=TEMPERATURE,        \n",
    "    max_tokens=MAX_TOKENS,        \n",
    "    openai_api_key=openai_api_key  \n",
    ")\n",
    "\n",
    "# Inicializamos también el modelos de embeddings seleccionado.\n",
    "# embeddings = OpenAIEmbeddings(    # (openai embeddings)\n",
    "#     model=EMBEDDING_MODEL,  \n",
    "#     openai_api_key=openai_api_key    \n",
    "# )\n",
    "\n",
    "#  alternativa para obtener embeddings\n",
    "\n",
    "# Se inicializa ChromaDB para almacenamiento vectorial persistente\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "vector_store = chroma_client.get_or_create_collection(\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función que permita monitorizar el coste de la llamada a la API de OpenAI.\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def llamada_monitorizada(llm, texto):\n",
    "    \"\"\" Función que realiza una llamada al modelo de lenguaje especificado y monitoriza el coste de la llamada.\n",
    "    Args:\n",
    "        llm (OpenAI): Modelo de lenguaje\n",
    "        texto (str): Texto de entrada\n",
    "\n",
    "    Returns:\n",
    "        response (str): Texto de salida generado por el modelo de lenguaje\n",
    "    \"\"\"\n",
    "    \n",
    "    with get_openai_callback() as callback:\n",
    "        response = llm(texto)\n",
    "        \n",
    "        # Detalles de uso y coste de la llamada.\n",
    "        print(f\"Tokens usados: {callback.total_tokens}\")\n",
    "        print(f\"Coste de la llamada: ${callback.total_cost:.5f}\")\n",
    "        \n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de llamada monitorizada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaración de herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def buscar_publicaciones_arxiv(consulta: str, max_resultados: int = 3) -> object:\n",
    "    \"\"\"\n",
    "    Busca publicaciones científicas en arXiv según una consulta del usuario.\n",
    "    \n",
    "    Args:\n",
    "        consulta (str): El término de búsqueda para arXiv.\n",
    "        max_resultados (int): Número máximo de publicaciones a devolver.\n",
    "\n",
    "    Returns:\n",
    "        PDFs de las publicaciones encontradas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Iinicializamos el cliente de arXiv\n",
    "        arxiv_client = arxiv.Client()\n",
    "\n",
    "        # Realizar la consulta a arXiv\n",
    "        search = arxiv.Search(\n",
    "            query=consulta,\n",
    "            max_results=max_resultados,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        \n",
    "        results = arxiv_client.results(search)\n",
    "        \n",
    "        if not results:\n",
    "            return \"No se encontraron publicaciones.\"\n",
    "        \n",
    "        for paper in results:\n",
    "            print(paper.title)\n",
    "            \n",
    "        return results\n",
    "          \n",
    "    except Exception as e:\n",
    "        return f\"Error al buscar en arXiv: {str(e)}\"\n",
    "\n",
    "\n",
    "def store_papers_in_chroma(papers, emb_model, collection):\n",
    "\n",
    "    # Configurar el text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    for paper in papers:\n",
    "        title = paper.get(\"title\", \"Unknown Title\")\n",
    "        authors = \", \".join(paper.get(\"authors\", []))\n",
    "        chapters = paper.get(\"chapters\", {})  # Ahora 'chapters' es un diccionario con capítulos y contenido\n",
    "        \n",
    "        for chapter, content in chapters.items():\n",
    "            # Crear el contenido del capítulo\n",
    "            full_text = f\"Title: {title}\\nAuthors: {authors}\\nChapter: {chapter}\\n\\n{content}\"\n",
    "            \n",
    "            # Dividir en chunks\n",
    "            chunks = text_splitter.split_text(full_text)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{title.replace(' ', '_')}_{chapter.replace(' ', '_')}_{i}\"\n",
    "                embedding = emb_model.embed_query(chunk)\n",
    "                \n",
    "                # Almacenar en ChromaDB\n",
    "                collection.add(\n",
    "                    ids=[chunk_id],\n",
    "                    embeddings=[embedding],\n",
    "                    metadatas=[{\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"chapter\": chapter,\n",
    "                        \"chunk_index\": i\n",
    "                    }],\n",
    "                    documents=[chunk]\n",
    "                )\n",
    "    \n",
    "    print(f\"Stored {len(papers)} papers in ChromaDB collection '{collection_name}'.\")\n",
    "\n",
    "\n",
    "def retrieve_similar_chunks(query, collection, emb_model, n_results=5):\n",
    "    \"\"\"\n",
    "    Realiza RAG sobre la base de datos de ChromaDB buscando los n chunks más similares al texto de entrada.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): Texto de búsqueda.\n",
    "    - collection (str): Nombre de la colección en ChromaDB.\n",
    "    - n_results (int): Número de resultados más similares a devolver.\n",
    "    -\n",
    "    \n",
    "    Returns:\n",
    "    - List[dict]: Lista con los chunks más relevantes y sus metadatos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generar embedding de la consulta\n",
    "        query_embedding = emb_model.embed_query(query)\n",
    "        \n",
    "        # Buscar en ChromaDB\n",
    "        results = collection.query(query_embeddings=[query_embedding], n_results=n_results)\n",
    "        \n",
    "        # Extraer documentos y metadatos\n",
    "        retrieved_chunks = []\n",
    "        for i in range(len(results[\"ids\"][0])):\n",
    "            retrieved_chunks.append({\n",
    "                \"chunk\": results[\"documents\"][0][i],\n",
    "                \"title\": results[\"metadatas\"][0][i][\"title\"],\n",
    "                \"authors\": results[\"metadatas\"][0][i][\"authors\"],\n",
    "                \"chapter\": results[\"metadatas\"][0][i][\"chapter\"]\n",
    "            })\n",
    "        \n",
    "        return retrieved_chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener información de la bbdd: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "def publicar_en_x(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Publica un texto en X (Twitter) utilizando la API de Tweepy.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): El texto que se desea publicar en X.\n",
    "    Returns:\n",
    "        str: Mensaje de éxito o error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        twitter_api_key = os.getenv(\"TWITTER_API_KEY\")\n",
    "        twitter_api_secret = os.getenv(\"TWITTER_API_SECRET\")\n",
    "        twitter_access_token = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "        twitter_access_token_secret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "        bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "        client_id = os.getenv(\"CLIENT_ID\")\n",
    "        client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "        # v2.0\n",
    "        auth = tweepy.OAuth2UserHandler(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, scope=\"tweet.write\")\n",
    "        print(auth)\n",
    "\n",
    "        client_X = tweepy.Client(\n",
    "                consumer_key=twitter_api_key,\n",
    "                consumer_secret=twitter_api_secret,\n",
    "                access_token=twitter_access_token,\n",
    "                access_token_secret=twitter_access_token_secret,\n",
    "            )\n",
    "\n",
    "        response = client_X.create_tweet(text=text, user_auth=True)\n",
    "        print(f\"Tweet publicado exitosamente: {response.data}\")\n",
    "        \n",
    "        return response.status_code #Añadir enlace al perfil de x\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error al publicar el tweet: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "\n",
    "\n",
    "class SearchOnArxivTool(BaseTool):\n",
    "    \"\"\"Herramienta de búsqueda de artículos científicos en arXiv.\"\"\"\n",
    "\n",
    "    name: str = \"SearchOnArxivTool\"\n",
    "    description: str = \"Searches for scientific articles on arXiv.\"\n",
    "    \n",
    "    def _run(self, consulta: str, max_resultados: int = 3) -> str:\n",
    "        \"\"\"Synchronous logic for executing the tool.\"\"\"\n",
    "        try:\n",
    "            return buscar_publicaciones_arxiv(consulta, max_resultados)\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    async def _arun(self, consulta: str, max_resultados: int = 3) -> str:\n",
    "        \"\"\"Asynchronous logic for executing the tool.\"\"\"\n",
    "        raise NotImplementedError(\"This tool does not support asynchronous operations.\")\n",
    "\n",
    "\n",
    "class TwitterPostTool(BaseTool):\n",
    "    \"\"\"Herramienta para publicar texto en X(Twitter).\"\"\"\n",
    "    \n",
    "    name: str = \"TwitterPostTool\"\n",
    "    description: str = (\n",
    "        \"Posts a message on X(Twitter). \"\n",
    "        \"Use this to share a message given a in your X account.\"\n",
    "    )\n",
    "    \n",
    "    def _run(self, texto: str) -> str:\n",
    "        \"\"\"Synchronous logic for executing the tool.\"\"\"\n",
    "        try:\n",
    "            return publicar_en_x(texto)\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "    async def _arun(self, texto: str) -> str:\n",
    "        \"\"\"Asynchronous logic for executing the tool.\"\"\"\n",
    "        raise NotImplementedError(\"This tool does not support asynchronous operations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario con las herramientas disponibles.\n",
    "herramientas_disponibles = {\n",
    "    \"arxiv_search\": SearchOnArxivTool(),\n",
    "    \"rag\": RetrieveTool(),\n",
    "    \"twitter_publisher\": TwitterPostTool()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de los nodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TypeDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMessage, HumanMessage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tool\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRouter\u001b[39;00m(TypeDict):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clase para el nodo que enruta las solicitudes a las herramientas correspondientes.\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mnext\u001b[39m: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearchOnArxivTool\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrivalDocumentInformation\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTwitterPostTool\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINISH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TypeDict' is not defined"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Literal, Sequence\n",
    "from langchain.schema import BaseMessage, HumanMessage\n",
    "from langchain.tools import Tool\n",
    "\n",
    "\n",
    "# Función para crear agentes genérica (Subgrafo que solo permite el uso de una herramienta por agente).\n",
    "def create_agent(llm, tool):\n",
    "    llm_with_tools = llm.bind_tools([tool])\n",
    "    def agent_function(state: AgentState):\n",
    "        return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "    \n",
    "    graph_builder = StateGraph(AgentState)\n",
    "    graph_builder.add_node(\"agent\", agent_function)\n",
    "    graph_builder.set_entry_point(\"agent\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Create supervisor node function\n",
    "def supervisor_node(state: MessagesState) -> Command[Literal[\"web_researcher\", \"rag\", \"nl2sql\", \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "    goto = response[\"next\"]\n",
    "    print(f\"Next Worker: {goto}\")\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "    return Command(goto=goto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Literal, Sequence\n",
    "from langchain.schema import BaseMessage, HumanMessage\n",
    "\n",
    "class Router(TypeDict):\n",
    "    \"\"\"Clase para el nodo que enruta las solicitudes a las herramientas correspondientes.\"\"\"\n",
    "    \n",
    "    next: Literal[\"SearchOnArxivTool\", \"RetrivalDocumentInformation\" \"TwitterPostTool\", \"FINISH\"]\n",
    "    \n",
    "\n",
    "class AgentState(TypeDict):\n",
    "    \"\"\"Clase para el estado del agente.\"\"\"\n",
    "    \n",
    "    messages: Sequence[BaseMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo Multiagente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StateGraph.add_edge() missing 1 required positional argument: 'end_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m builder \u001b[38;5;241m=\u001b[39m StateGraph(MessagesState)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Añadimos los nodos de las herramientas al grafo.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m builder\u001b[38;5;241m.\u001b[39madd_edge(START)\n\u001b[0;32m     10\u001b[0m graph \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mcompile()\n",
      "\u001b[1;31mTypeError\u001b[0m: StateGraph.add_edge() missing 1 required positional argument: 'end_key'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "# Declaración de los elementos del grafo.\n",
    "search_on_arxiv_node = create_agent(llm, tools[\"arxiv_search\"])\n",
    "retrival_document_information_node = create_agent(llm, tools[\"rag\"])\n",
    "twitter_post_node = create_agent(llm, tools[\"twitter_publisher\"])\n",
    "\n",
    "# Generación del grafo.\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Añadimos los nodos de las herramientas al grafo.\n",
    "builder.add_edge(START,\"supervisor\")\n",
    "\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_edge(\"SearchOnArxivTool\", search_on_arxiv_node)\n",
    "builder.add_edge(\"RetrivalDocumentInformation\", retrival_document_information_node)\n",
    "builder.add_edge(\"TwitterPostTool\", twitter_post_node)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización del Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al mostrar la imagen: 'Graph' object has no attribute 'get_graph'\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Error al mostrar la imagen: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se incializa el cliente y se crea una BBDD (collection)\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Crea la colección\n",
    "collection = chroma_client.create_collection(name=\"arxiv\")\n",
    "\n",
    "# Añade documentos a la colección\n",
    "collection.add(\n",
    "    documents=[\"ejemplo número 1\", \"ejemplo número 2\", \"ejemplo número 3\"],\n",
    "    metadatas=[{\"source\": \"arxiv\"}, {\"source\": \"arxiv\"}, {\"source\": \"arxiv\"}],\n",
    "    ids=[\"1\", \"2\", \"3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Front "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adria\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def yes_man(message, history):\n",
    "    if message.endswith(\"?\"):\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"Ask me anything!\"\n",
    "    \n",
    "def user_interface(user_input, history=[]):\n",
    "    state= {\"messages\": history + [HumanMessage(content=user_input)]}\n",
    "    results = graph.invoke(state)\n",
    "    response = results[\"messages\"][-1].content\n",
    "    history.append((user_input, response))\n",
    "    return response, history\n",
    "   \n",
    "\n",
    "demo = gradio.ChatInterface(\n",
    "    fn=yes_man,\n",
    "    title=\"MULTIAGENTE ARXIV RAG\",\n",
    "    description=\"Say hello to someone!\",\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
